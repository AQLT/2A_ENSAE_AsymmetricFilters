---
title: "Rapport"
author: "Alain Quartier-la-Tente"
date: "6/17/2020"
output:
  bookdown::pdf_document2:
        toc: true
        toc_depth: 2
        number_sections: true
        fig_width: 7
        fig_height: 6
        fig_caption: true
        highlight: default
        keep_tex: yes
themeoptions: "coding=utf8,language=french"
classoption: 'french'
lang: "french"
fontsize: 12pt
header-includes:
- \usepackage{setspace}
- \onehalfspacing
- \usepackage{mathptmx}
- \usepackage{multicol}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.path = "img/")
```

## Les différents noyaux

Dans l'extraction du signal, on cherche généralement à pondérer les observations selon leur distance à la date $t$. 
Pour cela on introduit une fonction de noyau $\kappa_j$, $j=0,\pm1,\dots,\pm h$ avec $\kappa_j \geq0$ et $\kappa_j=\kappa_{-j}$.
Une classe importante des noyaux qui comprend la majorité de ceux utilisé est la classe des Beta kernels :
$$
\kappa(u)=k_rs\left(1-\lvert u\rvert^r
\right)^s
1_{\lvert u\rvert\leq 1}
,\quad k_{rs}=\frac{r}{
2B\left(s+1,\frac 1 r\right)
}
$$
Avec $r>0$, $s\geq 0$ et 
$$
B(a,b)=\int_0^1u^{a-1}(1-u)^{b-1}du
$$
Dans le cas discret, à un facteur proportionnel près (constante de normalisation pour que $\sum\kappa_j=1$) :
$$
\kappa_j = \left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^r
\right)^s
$$
\begin{multicols}{2}
\begin{itemize}
\item $r=1,s=0$ uniform kernel : 
$$\kappa_j^U=1$$
%$$\kappa_j^U=\frac{1}{2h+1}$$

\item $r=s=1$ triangle kernel :
$$\kappa_j^T=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert
\right)$$
%$$\kappa_j^T=\frac{\left(
%  1-
%  \left\lvert
%  \frac j {h+1}
%  \right\lvert
%\right)}{
%h+1
%}
%$$

\item $r=2,s=1$  Epanechnikov (ou Parabolic) kernel :
$$\kappa_j^E=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
\right)$$
\end{itemize}
\end{multicols}


\begin{multicols}{2}
\begin{itemize}

\item $r=s=2$ biweight kernel :
$$\kappa_j^{BW}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
\right)^2$$

\item $r = 2, s = 3$ triweight kernel :
$$\kappa_j^{TW}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
\right)^3$$

\item $r = s = 3$ tricube kernel :
$$\kappa_j^{TC}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^3
\right)^3$$

\item Gaussian kernel  :
$$
\kappa_j^G=\exp\left(
-\frac{
  j^2
}{
  4h^2
}\right)
$$
\item Henderson kernel :
$$
\kappa_{j}=\left[(h+1)^{2}-j^{2}\right]\left[(h+2)^{2}-j^{2}\right]\left[(h+3)^{2}-j^{2}\right]
$$

\item Trapezoidal kernel :
$$
\kappa_j^{TP}=
\begin{cases}
  \frac{1}{3(2h-1)} & \text{ si }j=\pm h 
  \\
  \frac{2}{3(2h-1)} & \text{ si }j=\pm (h-1)\\
  \frac{1}{2h-1}& \text{ sinon}
\end{cases}
$$

\end{itemize}
\end{multicols}

Soit $x\in ]0,1[$ et $f_x(a,b)=\left(1-x^{a}\right)^{b}$
on a :
\begin{align*}
\frac{\partial}{\partial a}f(a,b) &=-a\ln (x)x^a(1-x^{a})^{b}>0 \\
\frac{\partial}{\partial b}f(a,b)&=\ln(1-x^{a})(1-x^{a})^{b} <0
\end{align*}
Donc :
\begin{align*}
\kappa_j^{E}&\geq \kappa_j^{BW} \geq \kappa_j^{TW} \\
\kappa_j^{E}&\geq \kappa_j^T \\
\kappa_j^{TC} &\geq \kappa_j^{TW}
\end{align*}

### Cas particuliers

Lorsque $p=0$ (ajustement local par une constante) on retrouve l'estimateur de Nadaraya-Watson

Avec le noyau uniforme, le filtre trouvé est le filtre de Macaulay. Dans le cas où $p=0,1$, ce filtre est la moyenne mobile arithmétique : $w_j=w=\frac{1}{2h+1}$.

Epanechnikov utilise le filtre suivant :
$$\kappa_j^E=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
\right)$$
Ce noyau est souvent recommandé comme le noyau optimal minimisant la moyenne quadratique d'une estimation par polynôme local.

Loess est une régression polynomiale locale pondérée utilisant le noyau tricube. Sa particularité est qu'il utilise la bande passante du plus proche voisin.

Le filtre de Henderson est un cas particulier de l'ajustement cubique $p=3$.
Henderson a cherché la fonction de poids qui donnait l'estimation la plus lisse de la tendance pour $h$ fixé et $p=3$.
Le critère de lissage utilisé par Henderson est basé sur la variance des différences d'ordre 3 des estimations de la tendances
Le critère de lissage adopté par Henderson est basé sur la variance des troisièmes différences des estimations de la tendance. Plus la variance est petite, plus le degré de lissage est élevé, car l'accélération de la tendance est sujette à moins de variations.

