---
title: "Rapport"
author: "Alain Quartier-la-Tente"
date: "6/17/2020"
output:
  bookdown::pdf_document2:
        toc: true
        toc_depth: 3
        number_sections: true
        fig_width: 7
        fig_height: 6
        fig_caption: true
        highlight: default
        keep_tex: yes
        includes:
          in_header: preamble.tex
          after_header: \thispagestyle{empty}
        citation_package: biblatex
themeoptions: "coding=utf8"
classoption: 'a4paper'
lang: "english"
fontsize: 12pt
bibliography: [biblio.bib]
biblio-style: authoryear
urlcolor: blue
---

<!-- output: -->
<!--   bookdown::word_document2: -->
<!--     toc: true -->
<!-- always_allow_html: true -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.path = "img/")
library(kableExtra)
```

\newpage

# Abstracts {.unnumbered}


\begin{abstract}
In the business cycle analysis, estimates are usually derived from moving average (also called linear filters) techniques. 
In the center of the series, symmetric filters are applied. 
However, due to the lack of future observations, real-time estimates must rely on asymmetric moving averages.
Classic asymmetric moving averages minimize revisions errors but introduce delays in the detecting turning points.

This paper describes and compares different approaches to build asymmetric filters: local polynomials filters, methods based on an optimization of filters' properties (Fidelity-Smoothness-Timeliness, FST, approach and a data-dependent filter) and filters based on Reproducing Kernel Hilbert Space.
It also describes how local polynomials filters can be extended to include a timeliness criterion to minimize phase shift.

This paper shows that constraining asymmetric filters to preserve constant and linear trends only (and not necessarily polynomial ones) reduce revision error and time lag. 
Besides, the more future observations are available, the more the different methods produce similar results.
Finally, the FST approach seems to outperform the other methods.
\end{abstract}

\renewcommand{\abstractname}{Résumé}

\begin{abstract}
Les moyennes mobiles (ou les filtres linéaires) sont omniprésents dans les méthodes d'extraction du cycle économique.
Au centre de la série, des filtres symétriques sont appliqués.
Cependant, en raison du manque d'observations futures, les estimations en temps réel doivent s'appuyer sur des moyennes mobiles asymétriques.
Les moyennes mobiles asymétriques classiques minimisent les erreurs de révision mais introduisent des retards dans la détection des points de retournement (déphasage).

Cet rapport décrit et compare différentes approches pour construire des filtres asymétriques: l'ajustement de polynôme locaux, des méthodes basées sur une optimisation des propriétés des filtres (l'approche *Fidelity-Smoothness-Timeliness* --- Fidélité-Lissage-Temporalité ---, FST, approche et un filtre dépendant des données) et des filtres basés sur les espaces de Hilbert à noyau reproduisant (RKHS).
Il décrit également comment les filtres polynomiaux locaux peuvent être étendus pour inclure un critère de temporalité afin de minimiser le déphasage.

Ce rapport montre que contraindre les filtres asymétriques à ne conserver que des les tendances constantes et linéaires (et pas nécessairement polynomiales) réduit l'erreur de révision et le décalage temporel.
Par ailleurs, plus d'observations futures sont disponibles, plus les différentes méthodes produisent des résultats similaires.
Enfin, l'approche FST semble l'emporter sur les autres méthodes.
\end{abstract}


\newpage

# Introduction {.unnumbered}

With the COVID-19 crisis, one of the major questions on the economy was when it will recover. 
It illustrates that business cycle analysis, and in particular the early detection of turning points in a series, is a major topic in the analysis of economic outlook.

Moving averages, or linear filters, are ubiquitous in business cycle extraction and seasonal adjustment methods^[
A moving average is a statistical method that consists in applying a rolling weighted mean to a times series: for each date it computes a weighted mean of $p$ past points and $q$ future points where $p,q\geq0$ depends on the moving average.
]. 
For example, the X-12ARIMA seasonal adjustment method uses Henderson moving averages and composite moving averages to estimate the main components of a time series, while TRAMO-SEATS uses Wiener-Kolmogorov filters. 
Symmetric filters are applied to the center of the series, but when it comes to estimate the most recent points, all of these methods must rely on asymmetric filters. 
For example, even if X-12ARIMA or TRAMO-SEATS apply symmetrical averages to the forecasts obtained from an ARIMA model of the series, in reality it consists in applying asymmetric filters at the end of the series, because the predicted values are linear combinations of past values.

If the classic asymmetric moving averages have good properties regarding the future revisions induced by the process^[See for example @pierce1980SA.], they create phase shifts that impact the real-time estimation of turning points, introducing time delay in the detection.

This report aims to describe and compare the recent approaches around trend-cycle extraction and asymmetric filters: local polynomial filters, methods based on an optimization of filters properties and filters based on Reproducing Kernel Hilbert Space.
Due to the link between seasonal adjustment and trend-cycle extraction (section \@ref(sec:SAtoTCE)), we focus on non-parametric methods that could be included in X-12ARIMA.
After a description of the general properties of a linear filter (section \@ref(sec:propMM)), we describe the linear filters methods developed by @proietti2008, @ch15HBSA, @trilemmaWMR2019 and @dagumbianconcini2008 (sections \@ref(sec:lppfilters) to \@ref(sec:Dagum)).
Finally, in section \@ref(sec:comparison), we compare the different methods, first theoretically and then applying them to a real time series.

\newpage

# From seasonal adjustment to trend-cycle estimation {#sec:SAtoTCE}

More and more infra-annual statistics are produced, especially by national institutes, to analyze the short-term evolution of economies. 
It is for example the case of the gross domestic product (GDP), unemployment rate, household consumption of goods and industrial production indices.
However, most of those time series are affected by seasonal and trading days effects. 
A seasonal effect is an effect that occurs in the same calendar month with similar magnitude and direction from year to year. 
For instance, automobile production is usually lower during summer, due to holidays, and chocolate sales are usually higher in December, due to Christmas. 
Trading days effect appears when a time series is affected by calendar month’s weekday composition. 
For example, retail sales are usually higher on Saturday, thus it is likely that they will be higher in months with a surplus of weekend days.

Seasonal and trading days effects can hamper the analysis of infra-annual movements of a time series or the spatial comparison. 
This is why time series are often seasonally and trading days adjusted, where seasonal adjustment is the process of removing the effects of seasonal and trading day fluctuations.

To perform seasonal adjustment, most of the algorithm decompose the data in several unknown components: trend-cycle, seasonal and irregular component.
In X-12ARIMA and TRAMO-SEATS (the most popular seasonal adjustment methods), prior to the decomposition, the initial series is pre-adjustment from deterministic effects (outliers, calendar effects).
Thus, the estimation of trend-cycle component is technically linked to the seasonal component.

Moreover, since the trend-cycle extraction methods are applied to seasonally adjusted data, both problems cannot be separated.
This link also explains that in this paper, all the methods used are implemented in the core libraries of JDemetra+ ^[See https://github.com/jdemetra/jdemetra-core.], the seasonal adjustment software recommended by Eurostat. 
An \faIcon{r-project} interface is implemented in the package `rjdfilters`^[Available at https://github.com/palatej/rjdfilters.] was developed during this internship.

In the literature, different approaches where considered for trend-cycle extraction^[See for example @alexandrov2012TEreview for a review of some modern approaches to trend extraction.]. 
Among the most popular, we can cite the Model-Based Approach and the non-parametric extraction methods:

- The Model-Based Approach assumes the specification of a stochastic time series model for the trend (ARIMA model, state space model, etc.) and the estimates is obtained by minimizing a penalty function, generally the mean squared error.
This is for example the case of the Kalman filter, the Wiener-Kolmogorov filter (used in TRAMO-SEATS) and the Direct Filter Approach of @trilemmaWMR2019 (section \@ref(sec:WildiMcLeroy)).

- The non-parametric extraction methods do not require specification of a model and can be easily applied to any time series. 
This is for example the case of the Henderson and Musgrave filters (used in X-12ARIMA).
Classical methods can be seen as local polynomial regression, approach generalized by @proietti2008 (section \@ref(sec:lppfilters)).  
Non-parametric estimators can also be reproduced by exploiting Reproducing Kernel Hilbert Space (RKHS) methodology, like it is done by 
@dagumbianconcini2008 (section \@ref(sec:Dagum)). 

@ch15HBSA (section \@ref(sec:GuggemosEtAl)) drawn a general unifying approach that allows a theoretical link between Musgrave non-parametric filter and the Direct Filter Approach.
They also proposed a global procedure to build asymmetric moving averages, that allows to minimize the phase shift effects and which is described in this report.


In this paper we focus methods that could be implemented in X-12ARIMA. 
To maintain consistency with the non-parametric approach of X-12ARIMA, we focus on non-parametric extraction methods. 
That's why neither the filters of the approach of  @trilemmaWMR2019 (section \@ref(sec:WildiMcLeroy)), nor other model based approaches  are used in this report.

Furthermore, for simplicity the data used is seasonally adjusted: the study should be extended to see the impact on the overall seasonal adjustment process.


# Moving average and filters {#sec:propMM}

Lots of papers describe the definition and the properties of moving average and linear filters (see for example @ch12HBSA). 
In this section we summarize some of the main results.

Let $p$ et $f$ two integers, a moving average $M_\theta$ or $M$ is defined by a set of coefficients $\theta=(\theta_{-p},\dots,\theta_{f})'$ such as for all time series $X_t$:
$$
M_\theta(X_t)=\sum_{k=-p}^{+f}\theta_kX_{t+k}
$$

- $p+f+1$ is called the *moving average order*.

- When $p=f$ the moving average is said to be *centered*. 
If we also have $\forall k:\:\theta_{-k} = \theta_k$, the moving average $M_\theta$ is said to be *symmetric*. 
In this case, the quantity $h=p=f$ is called the *bandwidth*.


## Gain and phase shift functions

Let $X_t=\e^{-i\omega t}$. The result of the moving average $M_\theta$ in $X_t$ is:
$$
Y_t = M_{\theta}X_t = \sum_{k=-p}^{+f} \theta_k \e^{-i \omega (t+k)}
= \left(\sum_{k=-p}^{+f} \theta_k \e^{-i \omega k}\right)\cdot X_t.
$$
The function $\Gamma_\theta(\omega)=\sum_{k=-p}^{+f} \theta_k e^{-i \omega k}$ is called the *transfer function* or *frequency response function*^[
The frequency response function can equivalently be defined as $\Gamma_\theta(\omega)=\sum_{k=-p}^{+f} \theta_k e^{i \omega k}$ or $\Gamma_\theta(\omega)=\sum_{k=-p}^{+f} \theta_k e^{2\pi i \omega k}$.
].
It can be rewritten as:
$$
\Gamma_\theta(\omega) = G_\theta(\omega)\e^{-i\Phi_\theta(\omega)}
$$
where $G_\theta(\omega)=\lvert\Gamma_\theta(\omega)\rvert$ is the *gain* or *amplitude* function and $\Phi_\theta(\omega)$ is the *phase shift* or *time shift* function^[
This function is sometimes represented as $\phi_\theta(\omega)=\frac{\Phi_\theta(\omega)}{\omega}$ to mesure the phase shift in number of periods.
]. 
For all symmetric moving average we have $\Phi_\theta(\omega)\equiv 0 \pmod{\pi}$.

To sum up, applying a moving average to a harmonic times series affects it in in two different ways:

- by multiplying it by an amplitude coefficient $G_{\theta}\left(\omega\right)$;

- by "shifting" it in time by $\Phi_\theta(\omega)/\omega$, which directly affects the detection of turning points^[
When $\Phi_\theta(\omega)/\omega>0$ the time shift is positive: a turning point is detected with delay.
].

Example: with $M_{\theta_0}X_t=\frac{1}{2}X_{t-1}+\frac{1}{2}X_{t}$ we have:
$$
\Gamma_{\theta_0}(\omega)=\frac{1}{2}+\frac{1}{2}\e^{-i\omega}
=\lvert\cos(\omega/2)\rvert\e^{-i\frac{\omega}{2}}
$$
The figure \@ref(fig:exgainPhase) illustrates the gain and the phase shift for $\omega=\pi/2$ and $X_t=\sin(\omega t)$.

\begin{figure}[!ht]
\pgfplotsset{width=\textwidth,height=6cm,every axis legend/.append style={font=\footnotesize,
  at={(0.5,-0.1)},
  anchor=north}
    }
\begin{tikzpicture}
\begin{axis}[
legend columns=2,
legend style = {fill=none , fill opacity=0, draw opacity=1,text opacity=1},
xtick={0,3.14159,...,15.70795},
xticklabels={0,$\pi$,$2\pi$,$3\pi$,$4\pi$,$5\pi$} 
]
\addplot[domain=0:5*pi,smooth,samples=300]    plot (\x,{sin(\x * (pi/2) r)});
\addlegendentry{$X_t(\pi/2)$}
\addplot[domain=0:5*pi,smooth,samples=300, dashed]    
  plot (\x,{1/2*sin(\x* pi/2 r )+1/2*sin((\x -1) * pi/2 r)});
\addlegendentry{$M_{\theta_0}X_t(\pi/2)$}
\draw[<->](axis cs: 1.5,1)--(axis cs: 1.5,0.7071068)
  node[pos=0.5, right]{\scriptsize $G_{\theta_0}(\pi/2)$};
\draw[<->] (axis cs: 3, -0.70710680-0.05)--(axis cs: 3.5,-0.7071068-0.05) 
  node[pos=0.5, below right]{\scriptsize $\Phi_{\theta_0}(\pi/2)$};
\end{axis}
\end{tikzpicture}
\caption{Smoothing of the time series $X_t=\sin(\omega t)$ by the moving average $M_{\theta_0}X_t=\frac{1}{2}X_{t-1}+\frac{1}{2}X_{t}$ for $\omega=\pi/2$.}\label{fig:exgainPhase}
\end{figure}


## Desirable properties of a moving average

The moving average are often constructed under some specific constraints. 
In the report we will focus on two constraints:

- the preservation of certain kind of trends;

- the variance reduction.

### Trend preservation

It is often desirable for a moving average to conserve certain kind of trends.
A moving average $M_\theta$ conserve a function of the time $f(t)$ if $\forall t:\:M_\theta f(t)=f(t)$.

We have the following properties for the moving average $M_\theta$:

- To conserve a constant series $X_t=a$ we need
$$
\forall t:M_\theta(X_t)=\sum_{k=-p}^{+f}\theta_kX_{t+k}=\sum_{k=-p}^{+f}\theta_ka=a\sum_{k=-p}^{+f}\theta_k=a
$$
the sum of the coefficients of the moving average $\sum_{k=-p}^{+f}\theta_k$ must then be equal to $1$.

- To conserve a linear trend $X_t=at+b$ we need:
$$
\forall t:\:M_\theta(X_t)=\sum_{k=-p}^{+f}\theta_kX_{t+k}=\sum_{k=-p}^{+f}\theta_k[a(t+k)+b]=at\sum_{k=-p}^{+f}k\theta_k+b\sum_{k=-p}^{+f}\theta_k=at+b
$$
which is equivalent to:
$$
\sum_{k=-p}^{+f}\theta_k=1
\quad\text{and}\quad
\sum_{k=-p}^{+f}k\theta_k=0
$$
- In general, it can be shown that $M_\theta$ conserves a polynomial of degree $d$ if and only if:
$$
\sum_{k=-p}^{+f}\theta_k=1 
 \text{ and } 
\forall j \in \left\llbracket 1,d\right\rrbracket:\:
\sum_{k=-p}^{+f}k^j\theta_k=0
$$
- If $M_\theta$ is symmetric ($p=f$ and $\theta_{-k} = \theta_k$) and conserves polynomial of degree $2d$ then it also conserves polynomial of degree $2d+1$.

### Variance reduction

All time series are affected by noise that can blur the signal extraction.
Hence, we seek to reduce the variance of the noise.
The sum of the squares of the coefficients $\sum_{k=-p}^{+f}\theta_k^2$ is the *variance reduction* ratio.

Indeed, let $\{\varepsilon_t\}$ a sequence of independent random variables with $\E{\varepsilon_t}=0$, $\V{\varepsilon_t}=\sigma^2$.
$$
\V{M_\theta\varepsilon_t}=\V{\sum_{k=-p}^{+f} \theta_k \varepsilon_{t+k}}
= \sum_{k=-p}^{+f} \theta_k^2 \V{\varepsilon_{t+k}}=
\sigma^2\sum_{k=-p}^{+f} \theta_k^2
$$

## Real-time estimation and asymmetric moving average {#defAsymProb}

For symmetric filters, the phase shift function is equal to zero (modulo $\pi$).
Therefore, there is no delay in any frequency: that's why they are preferred to the asymmetric ones. 
However, they cannot be used in the beginning and in the end of the time series because no past/future value can be used. 
Thus, for real-time estimation, it is needed to build asymmetric moving average that approximate the symmetric moving average.

The approximation is summarized by quality indicators. 
In this paper we focus on the ones defined by @ch15HBSA and @trilemmaWMR2019 to build the asymmetric filters.

@ch15HBSA propose a general approach to derive linear filters, based on an optimization problem of three criteria: *Fidelity* ($F_g$, noise reduction), *Smoothness* ($S_g$) and *Timeliness* ($T_g$, phase shift between input and ouput signals). 
See section \@ref(sec:GuggemosEtAl) for more details.

@trilemmaWMR2019 propose an approach based on the decomposition of the mean squared error between the symmetric and the asymmetric filter in four quantities: *Accuracy* ($A_w$), *Timeliness* ($T_w$), *Smoothness* ($S_w$) and *Residual* ($R_w$).
See section \@ref(sec:WildiMcLeroy) for more details.

All the indicators are summarized in table \@ref(tab:QC).

\begin{table}[!ht]
$$\begin{array}{ccc}
\hline \text{Sigle} & \text{Description} & \text{Formula}\\
\hline b_{c} & \text{Constant bias} & \sum_{k=-p}^{+f}\theta_{k}-1\\
\hline b_{l} & \text{Linear bias} & \sum_{k=-p}^{+f}k\theta_{k}\\
\hline b_{q} & \text{Quadratic bias} & \sum_{k=-p}^{+f}k^{2}\theta_{k}\\
\hline F_{g} & \text{Variance reduction / Fidelity (Guggemos)} & \sum_{k=-p}^{+f}\theta_{k}^{2}\\
\hline S_{g} & \text{Smoothness (Guggemos)} & \sum_{j}(\nabla^{3}\theta_{j})^{2}\\
\hline T_{g} & \text{Timeliness (Guggemos)} & \int_{0}^{2\pi/12}\rho_{\theta}(\omega)\sin(\varphi_{\theta}(\omega))^{2}\ud\omega\\
\hline A_{w} & \text{Accuracy (Wildi)} & 2\int_0^{2\pi/12}\left(\rho_{s}(\omega)-\rho_{\theta}(\omega)\right)^{2}h_{RW}(\omega)\ud\omega\\
\hline T_{w} & \text{Timeliness (Wildi)} & 8\int_0^{2\pi/12} \rho_{s}(\omega)\rho_{\theta}(\omega)\sin^{2}\left(\frac{\varphi_s(\omega)-\varphi_\theta(\omega)}{2}\right)h_{RW}(\omega)\ud\omega\\
\hline S_{w} & \text{Smoothness (Wildi)} & 2\int_{2\pi/12}^{\pi}\left(\rho_{s}(\omega)-\rho_{\theta}(\omega)\right)^{2}h_{RW}(\omega)\ud\omega\\
\hline R_{w} & \text{Residual (Wildi)} & 8\int_{2\pi/12}^{\pi} \rho_{s}(\omega)\rho_{\theta}(\omega)\sin^{2}\left(\frac{\varphi_s(\omega)-\varphi_\theta(\omega)}{2}\right)h_{RW}(\omega)\ud\omega\\
\hline \\
\end{array} $$
\caption{Criteria used to check the quality of a linear filter defined by its coefficients $\theta=(\theta_k)_{-p\leq k\leq f}$ and its gain and phase shift function, $\rho_{\theta}$ and $\varphi_\theta$.} 
\label{tab:QC}
\footnotesize
\emph{Note: $X_g$ criteria are derived from \textcite{ch15HBSA} and $X_w$ criteria from \textcite{trilemmaWMR2019}.}

\emph{$\rho_s$ and $\varphi_s$ represent the gain and phase shift function of the Henderson symmetric filter.}

\emph{$h_{RW}$ is the spectral density of a random walk: $h_{RW}(\omega)=\frac{1}{2(1-\cos(\omega))}$.}
\end{table}

# Local polynomial filters {#sec:lppfilters}

In this section we detail the filters that arise from fitting a local polynomial to our time series, as described by @proietti2008. 
Local polynomial filters encompass classical filters like Henderson and Musgrave filters (see sections \@ref(sec:sympolyfilter) and \@ref(subsec:lppasymf)).

We assume that our time series $y_t$ can be decomposed as 
$$
y_t=\mu_t+\varepsilon_t
$$
where $\mu_t$ is the signal (trend) and $\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})$ is the noise^[The time series is therefore seasonally adjusted.]. 
We assume that $\mu_t$ can be locally approximated by a polynomial of degree $d$ of the time $t$ between $y_t$ and the neighboring observations $\left(y_{t+j}\right)_{j\in\left\llbracket -h,h\right\rrbracket}$. 
Then $\mu_t\simeq m_{t}$ with:
$$
\forall j\in\left\llbracket -h,h\right\rrbracket :\:
y_{t+j}=m_{t+j}+\varepsilon_{t+j},\quad m_{t+j}=\sum_{i=0}^{d}\beta_{i}j^{i}
$$
This signal extraction problem is then equivalent to the estimation of $m_t=\beta_0$. In matrix notation we can write:
$$
\underbrace{\begin{pmatrix}y_{t-h}\\
y_{t-(h-1)}\\
\vdots\\
y_{t}\\
\vdots\\
y_{t+(h-1)}\\
y_{t+h}
\end{pmatrix}}_{y}=\underbrace{\begin{pmatrix}1 & -h & h^{2} & \cdots & (-h)^{d}\\
1 & -(h-1) & (h-1)^{2} & \cdots & (-(h-1))^{d}\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & h-1 & (h-1)^{2} & \cdots & (h-1)^{d}\\
1 & h & h^{2} & \cdots & h^{d}
\end{pmatrix}}_{X}\underbrace{\begin{pmatrix}\beta_{0}\\
\beta_{1}\\
\vdots\\
\vdots\\
\vdots\\
\vdots\\
\beta_{d}
\end{pmatrix}}_{\beta}+\underbrace{\begin{pmatrix}\varepsilon_{t-h}\\
\varepsilon_{t-(h-1)}\\
\vdots\\
\varepsilon_{t}\\
\vdots\\
\varepsilon_{t+(h-1)}\\
\varepsilon_{t+h}
\end{pmatrix}}_{\varepsilon}
$$
Two parameters are crucial in determining the accuracy of the approximation:

- the degree $d$ of the polynomial;

- the number of neighbors $H=2h+1$ (or the *bandwidth* $h$).

In order to estimate $\beta$ we need $H\geq d+1$ and the estimation is done by the weighted least squares (WLS), which consists of minimizing the following objective function:
$$
S(\hat{\beta}_{0},\dots,\hat{\beta}_{d})=\sum_{j=-h}^{h}\kappa_{j}(y_{t+j}-\hat{\beta}_{0}-\hat{\beta}_{1}j-\dots-\hat{\beta}_{d}j^{d})^{2}
$$
where $\kappa_j$ is a set of weights called *kernel*. We have $\kappa_j\geq 0:\kappa_{-j}=\kappa_j$, and with $K=diag(\kappa_{-h},\dots,\kappa_{h})$, the estimate of $\beta$ can be written as $\hat{\beta}=(X'KX)^{1}X'Ky$. 
With $e_{1}=\begin{pmatrix}1&0&\cdots&0\end{pmatrix}'$, the estimate of the trend is:
$$
\hat{m}_{t}=e_{1}\hat{\beta}=w'y=\sum_{j=-h}^{h}w_{j}y_{t-j}\text{ with }w=KX(X'KX)^{-1}e_{1}
$$
To conclude, the estimate of the trend $\hat{m}_{t}$ can be obtained applying the symmetric filter $w$ to $y_t$^[
$w$ is symmetric due to the symmetry of the kernel weights $\kappa_j$.
].
Moreover, $X'w=e_{1}$ so:
$$
\sum_{j=-h}^{h}w_{j}=1,\quad\forall r\in\left\llbracket 1,d\right\rrbracket :\sum_{j=-h}^{h}j^{r}w_{j}=0
$$
Hence, the filter $w$ preserve deterministic polynomial of order $d$.

## Different kernels {#sec:kernels}

In signal extraction, observations are generally weighted according to their distance from time $t$: this is the role of the kernel function.
In the discrete case, a kernel function is a set of weights $\kappa_j$, $j=0,\pm1,\dots,\pm h$ with $\kappa_j \geq0$ and $\kappa_j=\kappa_{-j}$.
An important class of kernels is the Beta kernels. In the discrete case, up to a proportional factor (so that $\sum_{j=-h}^h\kappa_j=1$):
$$
\kappa_j = \left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^r
\right)^s
$$
with $r>0$, $s\geq 0$. 
It encompasses all kernels used in this report, except Henderson, trapezoidal and gaussian kernel.The following kernels are considered in this report:


\begin{multicols}{2}
\begin{itemize}
\item $r=1,s=0$ uniform kernel: 
$$\kappa_j^U=1$$
\item $r=s=1$ triangle kernel:
$$\kappa_j^T=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert
\right)$$

\item $r=2,s=1$  Epanechnikov (or Parabolic) kernel:
$$\kappa_j^E=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
\right)$$

\item $r=s=2$ biweight kernel:
$$\kappa_j^{BW}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
\right)^2$$

\item $r = 2, s = 3$ triweight kernel:
$$\kappa_j^{TW}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
\right)^3$$

\item $r = s = 3$ tricube kernel:
$$\kappa_j^{TC}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^3
\right)^3$$

\item Henderson kernel (see section \ref{sec:sympolyfilter} for more details):
$$
\kappa_{j}=\left[1-\frac{j^2}{(h+1)^2}\right]
\left[1-\frac{j^2}{(h+2)^2}\right]
\left[1-\frac{j^2}{(h+3)^2}\right]
$$
\item Trapezoidal kernel:
$$
\kappa_j^{TP}=
\begin{cases}
  \frac{1}{3(2h-1)} & \text{ if }j=\pm h 
  \\
  \frac{2}{3(2h-1)} & \text{ if }j=\pm (h-1)\\
  \frac{1}{2h-1}& \text{ otherwise}
\end{cases}
$$
\item Gaussian kernel\footnote{
In this report we arbirarily take $\sigma^2=0.25$.
}
$$
\kappa_j^G=\exp\left(
-\frac{
  j^2
}{
  2\sigma^2h^2
}\right)
$$
\end{itemize}
\end{multicols}

<!-- Let $x\in ]0,1[$ and $f_x(a,b)=\left(1-x^{a}\right)^{b}$. We have: -->
<!-- \begin{align*} -->
<!-- \frac{\partial}{\partial a}f(a,b) &=-a\ln (x)x^a(1-x^{a})^{b}>0 \\ -->
<!-- \frac{\partial}{\partial b}f(a,b)&=\ln(1-x^{a})(1-x^{a})^{b} <0 -->
<!-- \end{align*} -->
<!-- So: -->

Henderson, trapezoidal and gaussian kernel are very specific:

- The Henderson and trapezoidal kernel functions change with the bandwidth (the other kernel only depend on the ratio $j/h+1$).

- Other definitions of the trapezoidal and gaussian kernel can be used. 
The trapezoidal kernel is here considered because it corresponds to the filter used to extract the seasonal component in the X-12ARIMA algorithm. 
Therefore it is never used to extract trend-cycle component.

\begin{figure}[!ht]
\animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/kernels/}{2}{30} 
\caption{Coefficients of the different kernels for $h$ from 2 to 30.}\label{fig:kernels}\footnotesize
\emph{Note: to see the animation, the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader. 
Otherwise you will only be able to see the results for $h=2$.}
\end{figure}

The figure \@ref(fig:kernels) summarizes the coefficients of the different kernels.
Analyzing the coefficients we can already anticipate some properties of the associated filters:

- The triweight kernel has the narrowest distribution.
The narrowest a distribution is, the smallest the weights of furthest neighbors are: the associated filter should have a high weight in the current observation ($t$).

- For $h$ high the Henderson kernel is equivalent to the triweight kernel (since $h+1\sim h+2 \sim h+3$, $\kappa_j^H\sim\kappa_j^{TW}$), the associated filter should also be equivalent.
However, for $h$ small ($h\leq10$) the Henderson kernel is closer to the biweight kernel than to the triweight kernel.



### Specific symmetric filters {#sec:sympolyfilter}

When $p=0$ (local adjustment by a constant) we obtain the **Nadaraya-Watson**'s estimator.

With the uniform kernel we obtain the **Macaulay filter**. 
When $p=0,1$, this is the arithmetic moving average: $w_j=w=\frac{1}{2h+1}$.

The **Epanechnikov** kernel is often recommended as the optimal kernel that minimizes the mean square error of the estimation by local polynomial.

**Loess** is a locally weighted polynomial regression that uses tricube kernel.

The **Henderson filter** is a specific case of a local cubic fit ($p=3$), widely used for trend estimation (for example it's the filter used in the seasonal adjustment software X-12ARIMA). For a fixed bandwidth, Henderson found the kernel that gave the smoothest estimates of the trend. 
He showed that the three following problems were equivalent:

1. minimize the variance of third difference of the series by the application of the moving average;  
2. minimize the sum of squares of third difference of the coefficients of the filter, it's the *smoothness criterion*: $S=\sum_j(\nabla^{3}\theta_{j})^{2}$;  
3. fit a local cubic polynomial by weighted least squares, where the weights are chose to minimize the sum of squares of the resulting filter.

Resolving the last problem leads to the kernel presented in section \@ref(sec:kernels).


### Analysis of symmetric filters

In this section, all the filters are computed by local polynomial of degree $d=3$. 
The figure \@ref(fig:filterscoefs) plots the coefficients of the filters for the different kernels presented in different kernels presented in section \@ref(sec:kernels) and for different bandwidths $h$. 
The table \@ref(tab:varianceReductionSymmetricFilters) shows the variance reduction of the different filters.
We find the similar results than in section \@ref(fig:kernels):

- The triweight kernel gives the filter with the narrowest distribution.
The narrowest a distribution is, the higher the variance reduction should be.
Indeed, the distribution of the coefficients of the filter can be interpreted as the output signal of an additive outlier.
As a result, with a wide distribution, an additive outlier will be more persistent than with a narrow distribution.
Therefore, it's the triweight that has the higher variance reduction for all $h\leq30$.

- For $h$ small, the trapezoidal filter seems to produce similar results than the Epanechnikov one.

- For $h$ small the Henderson filter is closed to the biweight kernel, for $h$ high it is equivalent to the triweight kernel.

\begin{figure}[!ht]
\animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/symmetricFilters/}{2}{30}
\caption{Coefficients of symmetric filters computed by local polynomial of degree $3$, according to the differents kernels and for $h$ from 2 to 30.}\label{fig:filterscoefs}\footnotesize
\emph{Note: to see the animation, the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader.
Otherwise you will only be able to see the results for $h=2$.}
\end{figure}

```{r varianceReductionSymmetricFilters, echo = FALSE}
dataVRSF <- readRDS("data/var_red_sym_filters.RDS")
colnames(dataVRSF)[1] <- "$h$"
title <- "Variance reduction ratio ($\\sum\\theta_i^2$) of symmetric filters computed by local polynomial of degree $3$."
round(dataVRSF[dataVRSF[,1]%in%c(2:10,20,30),],2) %>% 
  kable(format.args = list(digits = 2), align = "c", booktabs = T, row.names = FALSE,
        escape = FALSE,caption = title) %>% 
  kable_styling(latex_options=c("striped", "scale_down", "hold_position")) %>%
  add_header_above(c(" " = 1, "Kernel" = ncol(dataVRSF)-1))
```

Moreover, we find that for all the filters, the coefficients decrease, when the distance to the central observation increases, until a negative value and then increase towards 0 (except for the uniform kernel).
Negative coefficients might be disturbing but they arise from the cubic polynomial constraints.
Indeed to preserve polynomial of degree 2 (and so 3) we need $\sum_{j=-h}^hj^2\theta_i=0$, which constraint some coefficients to be negative.
However, those negative coefficients are negligible compare to the central coefficients (they are more 80% smaller than the central coefficient for all kernels, except for uniform and trapezoidal with high bandwidth).


### Gain functions 

Figure \@ref(fig:filtersSymgains) plots the gain functions of the different filters. 
Gain functions are usually plotted between 0 and $\pi$. 
However, locally weighted polynomial regression are low-pass filters: they leave almost unchanged low frequency components (such as the trend) and attenuate high frequency fluctuations (noise). 
For a monthly data, a cycle of 3 years corresponds to the frequency $2\pi/36$ and a cycle of 7 years to the frequency  $2\pi/84$.
Therefore, an ideal pass-band filter will have a gain function equal to 1 for low frequency ($\leq 2\pi/36$) and equal to 0 for other frequencies.

When the bandwidth $h$ increases, the gain function decreases for low frequencies: short business cycles will then be attenuated.
For a fixed value of $h$, gaussian, Henderson and triweight filters will preserve more short business cycles than the other filters (especially uniform, trapezoidal and Epanechnikov). 
Moreover, the gain function of those filters decreases faster to zero with less fluctuations: it enhances the higher variance reduction ratio shown in table \@ref(tab:varianceReductionSymmetricFilters).

 
\begin{figure}[!ht]
\animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/symmetricFilters/gain}{2}{30}
\caption{Gain functions from 0 to $2\pi/12$ of symmetric filters computed by local polynomial of degree $3$, according to the differents kernels and for $h$ from 2 to 30.}\label{fig:filtersSymgains}\footnotesize
\emph{Note: the two horizontal lines corresponds to the frequencies $2\pi/84$ (cycle of 7 years) and $2\pi/36$ (cycle of 3 years).}

\emph{to see the animation, the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader.
Otherwise you will only be able to see the results for $h=2$.}
\end{figure}

Just analyzing the symmetric filters properties, there is no doubt that Henderson, triweight and biweight filters have similar properties and will perform better than the other kernel for trend-cycle extraction. 
The same results are found with asymmetric filters. 
Thus, in order to simplify the presentation analysis, in the next sections we will only show the results with the Henderson filter.


## Asymmetric filters

### Direct asymmetric filters (DAF)

As mentioned in section \@ref(defAsymProb), symmetric filters cannot be used in boundary points. For real-time estimation, three different approaches can be used:

1. Build an asymmetric filter fitting local polynomial to the available observations $y_{t}$ for $t\in\left\llbracket n-h,n\right\rrbracket$.

2. Apply the symmetric filter to the series extended by forecast (or backcast) $\hat{y}_{n+l\mid n},l\in\left\llbracket 1,h\right\rrbracket$.

3. Build an asymmetric filter which minimize the mean square revision error subject to polynomial reproducing constraints.

@proietti2008 show that the first two approaches are equivalent when the forecast is done by a polynomial extrapolation of order $d$ (forecasts generated with the same polynomial model than the symmetric filter). 
This is called the *direct asymmetric filter* (DAF).
Let $q$ be the number of available observations in the future: $q$ varies from 0 (real time filter) to $h$ (symmetric filter).

Rewriting the matrix $X$, $K$ $y$ in the following way:
$$
X=\begin{pmatrix}X_{p}\\
X_{f}
\end{pmatrix},\quad y=\begin{pmatrix}y_{p}\\
y_{f}
\end{pmatrix},\quad K=\begin{pmatrix}K_{p} & 0\\
0 & K_{f}
\end{pmatrix}
$$
where $y_{p}$ correspond to the available data and $y_{f}$ the missing data. 
The DAF $w_a$ and the forecast $\hat{y}_{f}$ can be written as:
$$
w_{a}=K_{p}X_{p}(X'_{p}K_{p}X_{p})^{-1}e_{1},
\quad
\hat{y}_{f}=X_{f}(X'_{p}K_{p}X_{p})^{-1}X_{p}'K_{p}y_{p}
$$
Moreover, we have the following results with the DAF $w_a$:

- it satisfies the same polynomial reproduction constraints as the symmetric filter (conserve polynomial of degree $d$). 
Thus, the bias in estimating an unknown function of time has the same order of magnitude as in the interior of time support.

- $w_a$ minimize the weighted distance (by the kernel function) between the asymmetric filter coefficients and the symmetric ones. 
Therefore, for the DAF it is equivalent to fit a local polynomial and to minimize the revisions

However, the weights $w_{a,0}$ of the DAF are highly concentrated in the current observation $t$ with an important change between $q=0$ (real-time filter) and $q=h$ (see figure \@ref(fig:filtersdafcoefs)). 
Moreover the real-time filter doesn't have a satisfying gain functions: it is closer to one for all the frequencies (it thus has a low noise reduction power).
Therefore, even if the real-time filter is unbiased (if the series is generated by a polynomial of degree $d$) it is at the expenses of a high variance. 

\begin{figure}[!ht]
\includegraphics[width=\textwidth]{img/daf/coef_gain_1}
\caption{Coefficients and gain function of direct asymmetric filters (DAF) computed by local polynomial of degree $3$ with the Henderson kernel for $h=6$.}\label{fig:filtersdafcoefs}\footnotesize
\end{figure}

<!-- \begin{figure}[!ht] -->
<!-- \animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/daf/coef_gain_}{1}{9} -->
<!-- \caption{Coefficients and gain function of direct asymmetric filters (DAF) computed by local polynomial of degree $3$ with the Henderson kerne for $h=6$.}\label{fig:filtersdafcoefs}\footnotesize -->
<!-- \emph{Note: to see the animation, the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader. -->
<!-- Otherwise you will only be able to see the results for the Henderson kernel.} -->
<!-- \end{figure} -->

For all the kernels, we find the same results as in @proietti2008:

- For a fixed value of $d$, the more the data is available ($q$ increases), the more the weight associated to the current observation $w_{a,0}$ decreases.

- For a fixed value of $h$ and $q$, $w_{a,0}$ increases exponentially with the polynomial degree $d$ (in particular, for $d=h$, $w_{a,0}=1$).


### General class of asymmetric filters {#subsec:lppasymf}

To deal with the problem of the variance of the estimates of the real-time filters, @proietti2008 suggest a general of asymmetric filters to make a tradeoff between bias and variance.
It is a generalisation of Musgrave asymmetric filters (used in the seasonal adjustment algorithm X-12ARIMA, see @musgrave1964set).

Here we consider that the data is generated by the model:
$$
y=U\gamma+Z\delta+\varepsilon,\quad
\varepsilon\sim\mathcal{N}(0,D)
$$
The goal is to find a filter $v$ which minimize the mean square revision error (with the symmetric filter $w$) subject to some constraints. 
The constraints are summarized by the matrix $U=\begin{pmatrix}U_{p}'&U_{f}'\end{pmatrix}'$ (with $U_p$ the available observations of the matrix $U$ for the asymmetric filter): $U_p'v=U'w$. 
The problem is equivalent to find $v$ that minimize:
\begin{equation}
\varphi(v)=
\underbrace{
  \underbrace{(v-w_{p})'D_{p}(v-w_{p})+
  w_{f}'D_{f}w_{f}}_\text{revision error variance}+
  \underbrace{[\delta'(Z_{p}'v-Z'w)]^{2}}_{biais^2}
}_\text{Mean square revision error}+
\underbrace{2l'(U_{p}'v-U'w)}_{\text{constraints}}
(\#eq:lppasym)
\end{equation}
with $l$ a vector of Lagrange multipliers.

When $U=X$ this is equivalent to the constraint to preserve polynomial of degree $d$: we find the direct asymmetric filters $w_a$ with $D=K^{-1}$.

When $U=\begin{pmatrix}1&\cdots&1\end{pmatrix}'$, $Z=\begin{pmatrix}-h&\cdots&+h\end{pmatrix}'$, $\delta=\delta_1$, $D=\sigma^2I$ and when the symmetric filter is the Henderson filter we obtain the Musgrave asymmetric filters. 
With this filter we assume that the data is generated by a linear process and that the asymmetric filters preserve constant signals ($\sum v_i=\sum w_i=1$). 
The asymmetric filters depends on the ratio $\delta_1/\sigma$, which is related to the "I-C" ratio $R=\frac{\bar{I}}{\bar{C}}=\frac{\sum\lvert I_t-I_{t-1}\rvert}{\sum\lvert C_t-C_{t-1}\rvert}$ ($\delta_1/\sigma=2/(R\sqrt{\pi})$), the ratio between the expected absolute difference of the irregular and of the trend-cycle.
In the seasonal adjustment method, the I-C ratio^[
To compute the I-C ratio, a first decomposition of the seasonally adjusted series is computed using a 13-term Henderson moving average.
] is used to determine the bandwidth to use for the Henderson filter. For monthly data:

- if $R<1$ a 9-term Henderson is used ($h=4$);

- if $1\leq R\leq3.5$ a 13-term Henderson is used ($h=6$);

- if $3.5< R$ a 23-term Henderson is used ($h=12$).

In this report, for simplicity we only consider 13-term symmetric filters: the ratio  $\delta^2/\sigma^2$ is fixed to $3.5$.

When $U$ corresponds to the first $d^*+1$ first the columns of $X$, $d^*<d$, the constraint is that the asymmetric filter should reproduce polynomial of degree $d^*$, the potential bias depends on the value of $\delta$.
This will reduce the variance at the expense of a bias: it is the idea followed by @proietti2008 to propose three class of asymmetric filters:

1. *Linear-Constant* (LC): $y_t$ linear ($d=1$) and $v$ preserves constant signals ($d^*=0$). 
We obtain Musgrave filters when the Henderson kernel is used.

2. *Quadratic-Linear* (QL): $y_t$ quadratic ($d=2$) and $v$ preserves linear signals ($d^*=1$).

3. *Cubic-Quadratic* (CQ): $y_t$ cubic ($d=3$) and $v$ preserves quadratic signals ($d^*=2$).


<!-- ```{r criteriaLp, echo = FALSE} -->
<!-- lp_diagnostics <- readRDS("data/lp_diagnostics.RDS") -->
<!-- title <- "Quality criteria of real-time filters ($q=0$) computed by local polynomial." -->
<!-- groupement <- table(factor(lp_diagnostics[,1],levels = unique(lp_diagnostics[,1]), ordered = TRUE)) -->
<!-- lp_diagnostics[,-1] %>%  -->
<!--   kable(format.args = list(digits = 3), align = "c", booktabs = T, row.names = FALSE, -->
<!--         escape = FALSE,caption = title) %>%  -->
<!--   kable_styling(latex_options=c(#"striped",  -->
<!--                                 "scale_down", "hold_position")) %>% -->
<!--   add_header_above(c(" " = 1, "Kernel" = ncol(lp_diagnostics)-2)) %>% -->
<!--   pack_rows(index = groupement) -->
<!-- ``` -->

The table \@ref(tab:criteriaLp) shows the quality criteria of the different methods with the Henderson kernel and $h=6$. 
For real-time filters ($q=0$), the more complex the filter is (in terms of polynomial preservation), the less the timeliness is and the more the fidelity/smoothness is: the reduction of the time-delay is at the expense of an increased variance. 
This change when $q$ increases: for $q=2$ the QL filter has a greater timeliness that the LC filter. 
This unexpected result underlines the fact that in the approach of @proietti2008, the timeliness is never set as a goal to minimize.


```{r criteriaLp, echo = FALSE}
lp_diagnostics <- readRDS("data/lp_diagnostics_henderson.RDS")
title <- "Quality criteria of asymmetric filters ($q=0,1,2$) computed by local polynomial with Henderson kernel for $h=6$ and $R=3.5$."
groupement <- table(lp_diagnostics[,1])
lp_diagnostics[,-1] %>% 
  kable(format.args = list(digits = 3), align = "c", booktabs = T, row.names = FALSE,
        escape = FALSE,caption = title) %>% 
  kable_styling(latex_options=c(#"striped", 
                                "scale_down", "hold_position")) %>%
  pack_rows(index = groupement, escape = FALSE)
```

Regarding the mean square revision error ($A_w+S_w+T_w+R_w$), LC and QL filters always gives better results than CQ and DAF filters. 
This "theorical" mean square revision error can be compared to the "empirical" one computed applying the filters to real data.
The table \@ref(tab:mseIPI) shows the average mean square revision error of the different filters applied to the Industrial production indices of the European Union between 2003 and 2019. 
The mean square revision errors are different in level but give same results comparing the different filters: this validate the formula used for the criteria $A_w,S_w,T_w,\text{ and }R_w$ (see section \@ref(sec:WildiMcLeroy)). 

Taking for the value $R$ the "I-C" ratio computed with the raw data gives the same summary results.

\begin{table}[!htb]
\caption{\label{tab:mseIPI}Mean square revision error of asymmetric filters ($q=0,1,2$) computed by local polynomial on the Industrial production indices of the European Union.}
\centering
\begin{tabular}[t]{cccc}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{2}{c}{Mean squared revision error} & \multicolumn{1}{c}{ } \\
\cmidrule(l{3pt}r{3pt}){2-3}
Method & 2007-2010 & 2003-2019 & $A_w+S_w+T_w+R_w$\\
\midrule
\addlinespace[0.3em]
\multicolumn{4}{l}{$q=0$}\\
\hspace{1em}LC & 1.54 & 0.855 & 1.543\\
\hspace{1em}QL & 2.04 & 1.560 & 2.068\\
\hspace{1em}CQ & 2.88 & 2.363 & 2.349\\
\hspace{1em}DAF & 3.03 & 2.489 & 2.290\\
\addlinespace[0.3em]
\multicolumn{4}{l}{$q=1$}\\
\hspace{1em}LC & 2.96 & 1.315 & 0.304\\
\hspace{1em}QL & 2.31 & 1.074 & 0.247\\
\hspace{1em}CQ & 2.62 & 1.342 & 0.660\\
\hspace{1em}DAF & 2.72 & 1.437 & 0.840\\
\addlinespace[0.3em]
\multicolumn{4}{l}{$q=2$}\\
\hspace{1em}LC & 6.78 & 2.774 & 0.040\\
\hspace{1em}QL & 7.45 & 3.074 & 0.101\\
\hspace{1em}CQ & 7.99 & 3.620 & 0.635\\
\hspace{1em}DAF & 7.94 & 3.618 & 0.742\\
\bottomrule
\end{tabular}

\emph{Note: the filters are computed with $h=6$ (13-terms symmetric filter) and $R=3.5$.}
\end{table}

\faArrowCircleRight{} All those results suggest focusing on LC and QL filters and to focus on asymmetric linear filters that preserve polynomial trends of degree equal or less than one.

The results for the different kernels can also be visualized in an online application available at https://aqlt.shinyapps.io/FiltersProperties/.

:::: {.summary data-latex="{Local polynomial filters}"}
**Advantages**:

- Simple models with an easy interpretation.

- The asymmetric linear filter is independent of the date of estimation. 
However, it depends on the data if we calibrate the filter with the "I-C" ratio.


**Drawbacks**:

- Timeliness is not controlled.
::::



# General optimization problem: FST filters {#sec:GuggemosEtAl}

## Description of the approach

@ch15HBSA defined a general approach to derive linear filters, based on an optimization problem of three criteria. 
Contrary to the local polynomial approach of @proietti2008, this method enables to control the size of the timeliness in order to minimize the phase-shift.

The following criteria are used:

- *Fidelity*, $F_g$: it's the variance reduction ratio. It is called "Fidelity" because we want the output signal to be as close as possible to the input signal where the noise component is removed
$$
F_g(\theta) = \sum_{k=-p}^{+f}\theta_{k}^{2}
$$
$F_g$ can be rewritten in a positive quadratic form: $F_g(\theta)=\theta'F\theta$ with $F$ the identity matrix of order $p+f+1$.

- *Smoothness*, $S_g$: it's the Henderson smoothness criterion (sum of the squared of the third difference of the coefficients of the filter). 
It measures the flexibility of the coefficient curve of a filter and the smoothness of the trend.
$$
S_g(\theta) = \sum_{j}(\nabla^{3}\theta_{j})^{2}
$$
$S_g$ could also be rewritten in a positive quadratic form: $S_g(\theta)=\theta'S\theta$ with $S$ a symmetric matrix of order $p+f+1$.


- *Timeliness*, $T_g$: it measures the phase shift between input and output signal for specific frequencies. 
When a linear filter is applied, the level input signal is also altered by the gain function. 
Therefore, it is natural to consider that the higher the gain is, the higher the phase shift impact is. 
That's why the timeliness criterion depends on the gain and phase shift functions ($\rho_\theta$ and $\varphi_{\theta}$), the link between both functions being made by a penalty function $f$.
$$
T_g(\theta)=\int_{\omega_{1}}^{\omega_{2}}f(\rho_{\theta}(\omega),\varphi_{\theta}(\omega))\ud\omega
$$
In this article we use $\omega_1=0$ and $\omega_2=2\pi/12$ (for monthly data): we focus on the phase shift impact on cycles of more than one year.
For the penalty function, we take $f\colon(\rho,\varphi)\mapsto\rho^2\sin(\varphi)^2$. 
Indeed, for this function, the timeliness criterion is analytically solvable ($T_g=\theta'T\theta$ with $T$ a square symmetric matrix of order $p+f+1$), which is better in a computational point of view.

The asymmetric filters are computed minimizing a weighted sum of the past three criteria, subject to some constraints. Those constraints are usually polynomial preservation.

$$
\begin{cases}
\underset{\theta}{\min} & J(\theta)=
\alpha F_g(\theta)+\beta S_g(\theta)+\gamma T_g(\theta)\\
s.t. & C\theta=a
\end{cases}
$$
The conditions $\alpha,\beta,\gamma\geq 0\text{ and }\alpha+\beta\ne 0$ guarantee that $J(\theta)$ is a strictly convex function: therefore the optimization problem has a unique solution.

The Henderson symmetric filters can for example be computed with 
$$C=\begin{pmatrix}
1 & \cdots&1\\
-h & \cdots&h \\
(-h)^2 & \cdots&h^2
\end{pmatrix},\quad
a=\begin{pmatrix}
1 \\0\\0
\end{pmatrix},\quad
\alpha=\gamma=0,\quad
\beta=1$$

This approach can be called as the "FST approach" in reference to the three indicators used in the optimization problem.
One drawback of this approach is that the different criteria are not normalized: their values cannot be compared with each other.
As a result, the value of weights has no sense.

:::: {.summary data-latex="{FST filters}"}
**Advantages**:

- The asymmetric linear filter is independent of the symmetric filter, the data and the date of estimation.

- Unique solution to the optimization problem.

- The approach can be customized adding new criteria.

**Drawbacks**:

- The different criteria are not normalized: the associated weights cannot be compared.
::::

## Extension with the revision criterion

The FST --- Fidelity-Smoothness-Timeliness --- approach is the only one that doesn't directly include a criterion on the revision error relative to a symmetric filter.  
This approach could be somewhat generalized in order to include the revision criterion, replacing in the orthogonal form $\theta$ by $(w-\theta)$ (with $w$ the symmetric filter), and $F$ by a matrix relative to the data (see @ch12HBSA).

In this paper, we consider the method that consists to extend the minimization problem of local polynomial filters adding the Timeliness criterion defined by @ch15HBSA^[This method is for example coded in Java by Jean Palate in https://github.com/palatej/jdemetra-core.].
Using the same notations as in section \@ref(subsec:lppasymf), $\theta=v$ and noting $g=v-w_p$, the Timeliness criterion can be rewritten:
$$
T_g(v)=v'Tv=g'Tg+2w_p'Tg+w_p'Tw_p
\quad(T\text{ being symmetric)}
$$
Moreover, the objective function $\varphi$ of equation \@ref(eq:lppasym) can be rewritten as:
\begin{align*}
\varphi(v)&=(v-w_{p})'D_{p}(v-w_{p})+
  w_{f}'D_{f}w_{f}+
  [\delta'(Z_{p}'v-Z'w)]^{2}+
2l'(U_{p}'v-U'w)\\
&=g'Qg-2Pg+2l'(U_{p}'v-U'w)+c\quad\text{with }
\begin{cases}
Q=D_p+Z_p\delta\delta'Z'_p \\
P=w_fZ_f\delta\delta'Z_p'\\
c\text{ a constant independent of }v
\end{cases}
\end{align*}

Adding the Timeliness criterion, it becomes:
$$
\widetilde\varphi(v)=g'\widetilde Qg-
2\widetilde Pg+2l'(U_{p}'v-U'w)+
\widetilde c\quad\text{with }
\begin{cases}
\widetilde Q=D_p+Z_p\delta\delta'Z'_p +\alpha_TT\\
\widetilde P=w_fZ_f\delta\delta'Z_p'-\alpha_Tw_pT\\
\widetilde c\text{ a constant independent of }v
\end{cases}
$$
where $\alpha_T$ is the weight associated to the Timeliness criterion. With $\alpha_T=0$ we find $\varphi(v)$.

The figures \@ref(fig:lppguglc) show the impact of $\alpha_T$ on the coefficients of the linear filter with the LC method:

- The more $\alpha_T$ increases, the more the coefficient associated to the current observation increases: this is what we expected.

- $\alpha_T$ impacts logarithmically the coefficients: we can restraint $\alpha_T$ to $[0,2000]$.

- As expected, including the timeliness criterion has more impact for the value of $q$ that gives filters with higher timeliness: it corresponds to $q\leq2$ for the LC method. 
For the QL method we find that $\alpha_T$ has an impact for medium values of $q$ ($2\leq q\leq4$).


\begin{figure}[!ht]
\animategraphics[autoplay,loop,width=\textwidth,controls]{0.5}{img/lppgug_lc_q}{0}{5} 
\caption{Impact of the timeliness weight ($\alpha_T$) on the coefficients of the local polynomial filter with the LC method with $h=6$, $R=3.5$ and the Henderson kernel.
}\label{fig:lppguglc}\footnotesize
\emph{Note: to see the animation, the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader. 
Otherwise you will only be able to see the results for $q=0$.}
\end{figure}

<!-- \begin{figure}[!ht] -->
<!-- \animategraphics[autoplay,loop,width=\textwidth,controls]{0.5}{img/lppgug_ql_q}{0}{5}  -->
<!-- \caption{Impact of the timeliness weight ($\alpha_T$) on the coefficients of the local polynomial filter with the QL method with $h=6$, $R=3.5$ and the Henderson kernel. -->
<!-- }\label{fig:lppgug2}\footnotesize -->
<!-- \emph{Note: to see the animation, the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader.  -->
<!-- Otherwise you will only be able to see the results for $q=0$.} -->
<!-- \end{figure} -->


# Data-dependent filter {#sec:WildiMcLeroy}

In @trilemmaWMR2019, the authors proposed a data-dependent approach to derive linear filters. They decompose the mean square revision error in a trilemma between three quantities: *accuracy*, *timeliness* and *smoothness*. 
Contrary to @ch12HBSA, this decomposition implies that the values of the three criteria are comparable.

Let:

- $\left\{ x_{t}\right\}$ be our input time series;

- $\left\{y_{t}\right\}$ the target signal, i.e. the result of a symmetric filter, and $\Gamma_s$, $\rho_s$ and $\varphi_s$ the associated frequency response, gain and phase shift functions.

- $\left\{\hat y_{t}\right\}$ an estimation of $\left\{y_{t}\right\}$, i.e. the result of an asymmetric filter (when not all observations are available), and $\Gamma_\theta$, $\rho_\theta$ and $\varphi_\theta$ the associated frequency response, gain and phase shift functions.

If we assume that $\left\{ x_{t}\right\}$ is weakly stationary with a continuous spectral density $h$, the mean square revision error, $\E{(y_{t}-\hat{y}_{t})^{2}}$, can be written as:
\begin{equation}
\E{(y_{t}-\hat{y}_{t})^{2}}=\frac{1}{2\pi}\int_{-\pi}^{\pi}\left|\Gamma_s(\omega)-{\Gamma_\theta}(\omega)\right|^{2}h(\omega)\ud\omega=\frac{1}{2\pi}\times2\times\int_{0}^{\pi}\left|\Gamma_s(\omega)-{\Gamma_\theta}(\omega)\right|^{2}h(\omega)\ud\omega
(\#eq:msedef)
\end{equation}
This equality can also be generalized to non-stationary integrated process (for example imposing cointegration between both signals and using pseudo-spectral density, see @optimrtfWMR2013).

We have:
\begin{align}
\left|\Gamma_s(\omega)-\Gamma_\theta(\omega)\right|^{2} & =\rho_s(\omega)^{2}+\rho_\theta(\omega)^{2}+2\rho_s(\lambda)\rho_\theta(\lambda)\left(1-\cos(\varphi_s(\omega)-\varphi_\theta(\omega)\right) \nonumber\\
 & =\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2}+4\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_s(\omega)-\varphi_\theta(\omega)}{2}\right)
 (\#eq:msedecomp)
\end{align}

The interval $[0,\pi]$ is then splitted in two: the pass-band $[0,\omega_1]$ (the frequency interval that contains the target signal) and the stop-band  $[\omega_1,\pi]$.

The mean squared error defined in equation \@ref(eq:msedef) can then be decomposed additively into four quantities:
\begin{align*}
Accuracy =A_w&= 2\int_0^{\omega_1}\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2}h(\omega)\ud\omega\\
Timeliness =T_w&= 8\int_0^{\omega_1}\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)h(\omega)\ud\omega\\
Smoothness =S_w&= 2\int_{\omega_1}^\pi\left(\rho_s(\omega)^{2}-\rho_\theta(\omega)\right)^{2}h(\omega)\ud\omega\\
Residual =R_w&= 8\int_{\omega_1}^\pi\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)h(\omega)\ud\omega\\
\end{align*}

:::: {.remark data-latex=""}
To have coherent definitions between all sections, the formulas of the four criteria slightly differ from the ones defined in @trilemmaWMR2019:

- in this paper the interval integrations is $[0,\pi]$ rather than $[-\pi;\pi]$ (the integrals are then only multiplied by 2 because all the functions are even);

- the pass-band interval is defined as the frequency interval that contains the target signals whereas in @trilemmaWMR2019 it depends on the gain function of the symmetric filer (pass-band$=\{\omega |\rho_s(\omega)\geq 0.5\}$).
:::: 


In general, the residual $R_w$ is small since $\rho_s(\omega)\rho_\theta(\omega)$ is close to 0 in the stop-band. 
Moreover, user priorities are rarely concerned about the time-shift properties of components in the stop-band. 
That's why, to derive linear filters the residual is not taken into account; and that's why the authors suggest minimizing a weighted sum of the first three indicators:
$$
\mathcal{M}(\vartheta_{1},\vartheta_{2})=\vartheta_{1}T_w(\theta)+\vartheta_{2}S_w(\theta)+(1-\vartheta_{1}-\vartheta_{2})A_w(\theta)
$$
One of the drawbacks of this method is that there is no guarantee that there is a unique solution. 


In this paper we focus in non-parametric approaches to derive linear filters. 
That's why this approach is not considered.
However, the decomposition of the mean squared error gave useful indicators to compare linear filters because, in contrary the one presented in section \@ref(sec:GuggemosEtAl), their values can be easily interpreted and compared to each other.

To have criteria that don't depend on the data (and the one defined in table \@ref(tab:QC)), we take for the symmetric filter the Henderson filter and we fix the spectral density to the one of a random walk:
$$
h_{RW}(x)=\frac{1}{2(1-\cos(x))}
$$

:::: {.summary data-latex="{Data-dependent filters}"}
**Advantages**:

- The values of the different criteria can be compared: the weights can be easily interpreted.

**Drawbacks**:

- Data-dependent filter: it depends on the symmetric filter, the data and the date of estimation.

- Some optimization problems might occur (several minimum, etc.).
::::

We could also use a non-parametric approach to build asymmetric using $h_{RW}$ for the spectral density of the series. 
However, due to a lack of time, this option is not considered in this study.


# Asymmetric filters and Reproducing Kernel Hilbert Space {#sec:Dagum}

Classical non-parametric filters (Henderson, LOESS, Hodrick-Prescott) can be characterized using the Reproducing Kernel Hilbert Space (RKHS) methodology, as described by @dagumbianconcini2008.
It allows them to develop to derive linear filters and associated asymmetric filters.


A RKHS is a Hilbert space characterized by a kernel that reproduces, via an inner product defined by a density function $f_0(t)$, every function of the space. 
Therefore, a kernel estimator $K_p$ of order $p$ (i.e.: that reproduce without distortion a polynomial trend of degree $p-1$) can be decomposed into the product of a reproducing kernel $R_{p-1}$, belonging to the space of polynomials of degree $p-1$, and a probability density function $f_0$.

For any sequence $\left(P_{i}\right)_{0\leq i\leq p-1}$ of orthonormal polynomials in $\mathbb{L}^{2}(f_{0})$^[
$\mathbb{L}^{2}(f_{0})$ is the Hilbert space defined by the inner product:
$$
\left\langle U(t),V(t)\right\rangle =\E{U(t)V(t)}=\int_{\R}U(t)V(t)f_{0}(t)\ud t
$$
], the kernel estimator $K_p$ is defined by:
$$
K_{p}(t)=\sum_{i=0}^{p-1}P_{i}(t)P_{i}(0)f_{0}(t)
$$
The weights of a symmetric filter are then derived with:
$$
\forall j\in\left\llbracket -h,h\right\rrbracket\::\: w_{j}=\frac{K_p(j/b)}{\sum_{i=-h}^{^h}K_p(i/b)}
$$
where $b$ is a time-invariant global bandwidth parameter.

The density $f_0$ corresponds to the continuous versions of the kernel defined in \@ref(sec:kernels). 
For example, the biweight function is $f_{0B}(t)=(15/16)(1-t^2)^2,t\in [-1,1]$.
The local polynomial filter obtained with the biweight kernel is then obtained using the bandwidth $b=h+1$.

The goal of @dagumbianconcini2008 is to derive asymmetric filters from the Henderson symmetric filter, therefore the ideal kernel function would be the Henderson one. 
However, as shown in section \@ref(sec:kernels), the Henderson density is a function of the bandwidth and needs to be calculated any time $m$ changes (as its corresponding orthonormal polynomial). 
That's why the authors use the biweight kernel to approximate the Henderson kernel (for $h\geq 24$ they suggest to consider the triweight kernel).

The asymmetric weighted are obtained adapting the kernels to the length of the asymmetric filters:
$$
\forall j\in\left\llbracket -h,q\right\rrbracket\::\: w_{a,j}=\frac{K_p(j/b)}{\sum_{i=-h}^{^q}K_p(i/b)}
$$
With $b=h+1$, @proietti2008 show that we obtain the direct asymmetric filters (DAF).

In @dagumbianconcini2015, the authors suggest performing an optimal bandwidth selection (parameter $b$), decomposing the mean squared revision error as in equation \@ref(eq:msedecomp) but with a uniform spectral density ($h(\omega)=1$).
The bandwidth can then be chosen to minimize the mean squared revision error, the phase shift, etc. 
The following bandwidth selection are studied:
\begin{align*}
b_{q,G}&=\underset{b_q\in]h;2 h+1]}{\min}
\sqrt{2\int_{0}^{\pi}
\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2}\ud \omega
}\\
b_{q,\gamma}&=\underset{b_q\in]h;2 h+1]}{\min}
\sqrt{2\int_{0}^{\pi}
\lvert \Gamma_s(\omega)-\Gamma_\theta(\omega)\rvert^2\ud \omega
} \\
b_{q,\varphi}&=\underset{b_q\in]h;2 h+1]}{\min}
8\int_{0}^{2\pi/12}
\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)\ud \omega
\end{align*}

One of the drawbacks of the optimal bandwidth selection is that there is no guarantee that there is a unique solution. 

:::: {.remark data-latex=""}
To have coherent definitions between all sections, the formulas of $b_{q,G}$, $b_{q,\gamma}$ and $b_{q,\varphi}$ slightly differ from the ones defined in @dagumbianconcini2015 where:

- $b_{q,\varphi}$ is defined as 
$$
b_{q,\varphi}=\underset{b_q\in]h;2 h+1]}{\min}
\sqrt{2\int_{\Omega_S}
\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)\ud \omega}
$$

With $\Omega_S=[0,2\pi/36]$ the frequency domain associated to cycles of 16 months or longer.

- a different formula is used for the frequency response function ($\Gamma_\theta(\omega)=\sum_{k=-p}^{+f} \theta_k e^{2\pi i \omega k}$): it only changes the interval integration.
::::


The table \@ref(tab:criteriarkhs) shows the quality criteria of the RKHS filters.
Even if the symmetric filter preserves quadratic trends, this is not the case of the asymmetric filters: they only preserve constant trends.
The more data is available (i.e.: $q$ increases), the less the asymmetric filters distort polynomial trends (for example, for $q=2$, $b_l\simeq0$ for $b_{q,G}$).

```{r criteriarkhs, echo = FALSE}
rkhs_diagnostics <- readRDS("data/rkhs_diagnostics.RDS")
title <- "Quality criteria of asymmetric filters ($q=0,1,2$) computed by the RKHS methodology $h=6$."
groupement <- table(rkhs_diagnostics[,1])
rkhs_diagnostics[,-1] %>% 
  kable(format.args = list(digits = 3), align = "c", booktabs = T, row.names = FALSE,
        escape = FALSE,caption = title) %>% 
  kable_styling(latex_options=c(#"striped", 
                                "scale_down", "hold_position")) %>%
  pack_rows(index = groupement, escape = FALSE)
```

:::: {.summary data-latex="{RKHS filters}"}
**Advantages**:

- The asymmetric linear filter is independent of the data and of the date of estimation.

- Filters to apply to irregular frequency series (for example with a lot of missing values) can easily be computed.

**Drawbacks**:

- The linear filters don't preserve polynomial trends of degree 1 or more.

- Some optimization problems might occur (several minimum, etc.).
::::

# Comparison of the different filters {#sec:comparison}

## Comparison with the FST approach

The FST approach provides a useful tool to validate a method of construction of linear filter. 
Indeed, a linear filter can be considered as suboptimal if we can find a set of weight for the FST approach that gives better results (in terms of fidelity, smoothness and timeliness) with the same (or higher) polynomial constraints.

For $h=6$ (13-term symmetric filter), for the RKHS filters we find that:

- imposing that asymmetric filters preserve constants, the FST approach gives better results for all values of $q$ for the filters computed with $b_{q,G}$ and $b_{q,\gamma}$, and for $q\leq 3$ for the one computed with $b_{q,\varphi}$.

- imposing that asymmetric filters preserve linear trends, the FST approach gives better results for $q\in[2,5]$ for the filters computed with $b_{q,G}$ and $b_{q,\gamma}$, and for $q\in[2,4]$ for the one computed with $b_{q,\varphi}$.

\faArrowCircleRight{} Therefore, the RKHS approach seems not to be the better approach to build asymmetric filters to apply to monthly regular data (i.e.: data for which we usually use $h=6$).

For the local polynomial filters we find that:

- imposing that asymmetric filters preserve constants, the FST approach gives better results than the LC method for all values of $q$. 
Adding the timeliness criterion allows to have better results than the FST approach for $q=0,1$.

- imposing that asymmetric filters preserve linear trends, the FST approach gives better results for $q\in[2,5]$ for the LC and QL methods. 
Adding the timeliness criterion doesn't change the results.

\faArrowCircleRight{} In terms of quality criteria, local polynomial seems to perform better for real-time and one-period ahead filters ($q=0,1$).

Those comparisons can be seen in an online application available at https://aqlt.shinyapps.io/FSTfilters/.

## Illustration with an example

In this section we compare the different asymmetric filters applying them to the French industrial production index of manufacture of motor vehicles, trailers and semi-trailers^[This series is produced by the INSEE, the French National Statistical Office, and is available at https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/010537940.]. 
For simplicity, for the FST approach we only show an extreme filter example minimizing the phase shift by putting a large weight to the timeliness ($\alpha = 0$, $\beta = 1$, $\gamma = 1000$). 
For local polynomial filters, the Henderson kernel is used with $R=3.5$.
The figure \@ref(fig:cl2all) shows the results around a turning point (the economic crisis of 2009) and during "normal cycles" (between 2000 and 2008), from $q=0$ (real-time estimates) to $q=3$ (three future available observations).

\begin{figure}[!ht]
\animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/illustration/illustration_}{1}{7}
\caption{Application of asymmetric filters to the French industrial production index of manufacture of motor vehicles, trailers and semi-trailers.}\label{fig:cl2all}\footnotesize
\emph{Note: The final trend is the one computed by a symmetric 13-terms Henderson filter}

\emph{To see the animation, the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader.
Otherwise you will only be able to see the results for the DAF filter.}
\end{figure}

For this example, in terms of time lag to detect the turning point of February 2009^[
A turning point (upturn) is defined to occur at time $t$ if $y_{t-k}\geq\cdots\geq y_{t-1}<y_t\leq y_{t+1}\leq\cdots y_{t+m}$. 
A downturn would be equivalently defined as a date $t$ $y_{t-k}\leq\cdots\leq y_{t-1}>y_t\geq y_{t+1}\geq\cdots y_{t+m}$.
Following @Zellner1991, $k=3$ and $m=1$ are chosen given the smoothness of the trend-cycle data. 
], the DAF is the worst filter since it introduces a delay of one month until $q=3$. 
LC, QL, $b_{q,\gamma}$ filters introduce a delay only for real-time filter ($q=0$) of two months for QL filter and one month for the others.
Surprisingly, the FST filter that minimize the phase shift introduce a time lag for $q=0$ and $q=1$ (the upturn is detected earlier), whereas $b_{q,\phi}$ doesn't introduce any time lag.

:::: {.remark data-latex=""}
Those results differ from the one obtained in the real-time during the economic crisis. 
Indeed, here we take the time series already seasonally adjusted in the overall period, whereas during the real-time estimates, the seasonal adjustment process also introduce a time lag.
::::


In terms of revision error, the DAF filters also give the worst results. 
For real-time estimates ($q=0$), the LC and RKHS filters minimize the revision between 2000 and 2008 but when more data are available all the methods (except the DAF filters) give similar results. 
As excepted, the QL filter minimizes the revision around the turning point of 2009 since it is the only filter that preserve polynomial trends of degree 1. 
For small values of $q$, $b_{q,\phi}$ filter distorts less polynomial trends of degree 1 and 2 than the other RKHS filters ($b_l$ and $b_q$ statistics are lower, table \@ref(tab:criteriarkhs)): it explains why it performs better around the turning point.

With this simple example we cannot generalize properties on the different asymmetric filters, but it gives some leads to extend this study:

- minimize the timeliness doesn't imply earlier detection of turning points: more investigations on the timeliness criterion could be done;

- accept some bias in polynomial trends preservation can lead to asymmetric filters that performs better in term of turning points detection;

- for almost all the filters, there is no time lag to detect the turning points when two future observations are available, and the different methods gives similar results. 
Therefore, it is pointless to optimize asymmetric filters when too much future observations are available.

Moreover, this example also illustrates that the different methods perform better than the DAF filters. 
The real-time estimates methods that use this filter could be easily improved.
This is for example the case of STL (Seasonal-Trend decomposition based on Loess) seasonal adjustment method proposed by @cleveland90.


<!-- \@ref(fig:filtersasymErr): -->

<!-- - almost same symmetric trend for all the filters: small differences during "turning points".  -->
<!-- Differences less than one point for between all the kernels, uniform and trapezoidal more "atypical", Differences less than 0,5 point for the other kernels. -->
<!-- Différences p/r Henderson toujours dans le même sens sauf pour triweight (?): sous Henderson pendant les "cycles bas", au-dessus pendant "cycles hauts" -->

<!-- - revisions error are higher with uniform and trapezoidal kernels, lower with triweight, regardless of the asymmetric filter. -->

<!-- - in mean, the variance revision error is always lower with the LC filter and higher with the DAF. However, during turning-points (for example around the 2008 crisis), the LC filter have a higher bias than the other filters. For the French IPI, the CQ method seems to give the better results around the 2008 crisis (analyzing mean squared error revision between 2007 and 2011). However, reproducing the same procedure to the IPI of other countries of the European Union (and the United-Kingdom), we find that for most of the series (23 of 28) its the QL filter that gives the better results during the 2008 crisis. -->

<!-- \begin{figure}[!ht] -->
<!-- \animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/daf/comp_assym_}{1}{9} -->
<!-- \caption{Revision error of the different real-time ($q=0$) asymmetric filters for $h=6$, for the IPI-FR.}\label{fig:filtersasymErr}\footnotesize -->
<!-- \emph{Note: to see the animation, the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader. -->
<!-- Otherwise you will only be able to see the results for the Henderson kernel.} -->
<!-- \end{figure} -->

\newpage

# Conclusion {.unnumbered}

To sum up, this study presents many results about how to build asymmetric filters to minimize revision and time shift in the detection of turning points.

First, there is no need to seek to preserve polynomial trends of degree more than one to build asymmetric filters: it introduces more bias in the detection of turning point with a low gain on the revision error. Moreover, introducing a small bias in polynomial preservation can lead to filters that performs globally better. 
This property has been illustrated empirically on the French IPI of manufacture of motor vehicles, trailers and semi-trailers in section \@ref(sec:comparison) using the RKHS $b_{q,\phi}$ filter.

Second, focusing on some specific criteria, the FST approach can lead to asymmetric filters that outperform the other ones (local polynomials and RKHS). 
More research should be done in that direction to understand when it performs better and if the improvement is statistically significant.

<!-- Third, all the methods lots of degrees of freedom on the criteria used to build them.  -->
<!-- Therefore, more theoretical and empirical studies should be done on the effects of this degrees of freedom to see how they impact the estimation and if we cannot simplify them, as it is done in the X-12ARIMA algorithm to choose the length of the Henderson filter.  -->

Finally, the study of asymmetric filters should focus on estimates when few future observations are available. 
Indeed, for monthly data, when two or more future observations are available, the different methods give similar results and the gain to compute optimized filters is small.
Consequently, adding the timeliness criterion to local polynomial filters will mainly be useful for the LC method, for which it has an impact only when few future observations are known (whereas for the QL method is has almost no impact on real-time estimates).

This study could be extended in many ways.
One lead could be to do more investigation on the length of the filters. 
In most of the studies on asymmetric filters, the asymmetric filters use as many past observations as the symmetric filters.
However, we could consider asymmetric filters that take into account more past observations. 
Moreover, in this paper we only focused on regular monthly data, but if we focus on high frequency or irregular data, some properties may not be effective anymore and, besides, some more issues could appear (as the problem of choosing the length of the filter).

Another lead could be to study the impact of atypical points: in X-12ARIMA there is a strong correction of atypical points of the irregular component before applying the filters to extract the trend. 
It encourages to study what is the impact of atypical points on the estimation of trends and turning points, but also to explore new kinds of asymmetric filters based on robust methods (like robust local regressions). 


<!-- une conclusion: elle doit situer le point d’arrivée, indiquer ce qui a pu être réalisé mais aussi ce qui n’a pu être entrepris. Elle peut aussi ouvrir la voie pour ce qui pourrait être la continuation du travail entrepris. -->


\newpage

# References {.unnumbered}



