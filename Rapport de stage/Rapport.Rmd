---
title: "Rapport"
author: "Alain Quartier-la-Tente"
date: "6/17/2020"
output:
  bookdown::pdf_document2:
        toc: true
        toc_depth: 3
        number_sections: true
        fig_width: 7
        fig_height: 6
        fig_caption: true
        highlight: default
        keep_tex: yes
        includes:
          in_header: preamble.tex
          after_header: \thispagestyle{empty}
        citation_package: biblatex
themeoptions: "coding=utf8"
classoption: 'a4paper'
lang: "english"
fontsize: 12pt
bibliography: [biblio.bib]
biblio-style: authoryear
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.path = "img/")
library(kableExtra)
```

\newpage

# Abstract

# Introduction


# From seasonal adjustment to trend-cycle estimation

Indiquer pourquoi on se place dans le cadre de la désaisonnalisation.

Dire que l'on se place dans le cadre de X-12-ARIMA qui est une méthode non-paramétrique : on va s'intéresser à deux approches : @proietti2008 (section \@ref(sec:lppfilters)) et @ch15HBSA (section \@ref(sec:GuggemosEtAl)).

Mentionner également @ch14HBSA (section \@ref(sec:Dagum})) et @ch16HBSA / @trilemmaWMR2019 (section \@ref(sec:WildiMcLeroy)).

Description du trilemma.

Parler aussi de la timeliness



# Moving average and filters {#sec:propMM}

A lot of papers describes the definition and the properties of moving average and linear filters (see for example @ch12HBSA). 
Here we summarize some of the main results.

Let $p$ et $f$ two integers, a moving average $M_\theta$ or $M$ is defined by a set of coefficients $\theta=(\theta_{-p},\dots,\theta_{f})'$ such as:
$$
M_\theta(X_t)=\sum_{k=-p}^{+f}\theta_kX_{t+k}
$$

- $p+f+1$ is called the *moving average order*.

- When $p=f$ the moving average is said to be *centered*. 
If we also have $\forall k:\:\theta_{-k} = \theta_k$, the moving average $M_\theta$ is said to be *symmetric*. 
In this case, the quantity $h=p=f$ is called the *bandwidth*.


## Gain and phase shift functions

Let $X_t=\e^{-i\omega t}$, the result of the moving average $M_\theta$ in $X_t$ is:
$$
Y_t = M_{\theta}X_t = \sum_{k=-p}^{+f} \theta_k \e^{-i \omega (t+k)}
= \left(\sum_{k=-p}^{+f} \theta_k \e^{-i \omega k}\right)\cdot X_t.
$$
The function $\Gamma_\theta(\omega)=\sum_{k=-p}^{+f} \theta_k e^{-i \omega k}$ is called the *transfer function* or *frequency response function*^[
The frequency response function can equivalently be defined as $\Gamma_\theta(\omega)=\sum_{k=-p}^{+f} \theta_k e^{i \omega k}$ or $\Gamma_\theta(\omega)=\sum_{k=-p}^{+f} \theta_k e^{2\pi i \omega k}$.
].
It can be rewritten as:
$$
\Gamma_\theta(\omega) = G_\theta(\omega)\e^{-i\Phi_\theta(\omega)}
$$
where $G_\theta(\omega)=\lvert\Gamma_\theta(\omega)\rvert$ is the *gain* or *amplitude* function and $\Phi_\theta(\omega)$ is the *phase shift* or *time shift* function^[
This function is sometimes represented as $\phi_\theta(\omega)=\frac{\Phi_\theta(\omega)}{\omega}$ to mesure the phase shift in number of periods.
]. 
For all symmetric moving average we have $\Phi_\theta(\omega)=0$.

To sum up, applying a moving average to an harmonic times series affects in in two different ways:

- by multiplying it by an amplitude coefficient $G_{\theta}\left(\omega\right)$;

- by "shifting" it in time by $\Phi_\theta(\omega)/\omega$, which directly affects the detection of turning points^[
When $\Phi_\theta(\omega)/\omega>0$ the time shift is positive: a turning point is detected with delay.
].

Example: with $M_{\theta_0}X_t=\frac{1}{2}X_{t-1}+\frac{1}{2}X_{t}$ we have:
$$
\Gamma_{\theta_0}(\omega)=\frac{1}{2}+\frac{1}{2}\e^{-i\omega}
=\lvert\cos(\omega/2)\rvert\e^{-i\frac{\omega}{2}}
$$
The figure \@ref(fig:exgainPhase) illustrates the gain and the phase shift for $\omega=\pi/2$ and $X_t=\sin(\omega t)$.

\begin{figure}[!ht]
\pgfplotsset{width=\textwidth,height=6cm,every axis legend/.append style={font=\footnotesize,
  at={(0.5,-0.1)},
  anchor=north}
    }
\begin{tikzpicture}
\begin{axis}[
legend columns=2,
legend style = {fill=none , fill opacity=0, draw opacity=1,text opacity=1},
xtick={0,3.14159,...,15.70795},
xticklabels={0,$\pi$,$2\pi$,$3\pi$,$4\pi$,$5\pi$} 
]
\addplot[domain=0:5*pi,smooth,samples=300]    plot (\x,{sin(\x * (pi/2) r)});
\addlegendentry{$X_t(\pi/2)$}
\addplot[domain=0:5*pi,smooth,samples=300, dashed]    
  plot (\x,{1/2*sin(\x* pi/2 r )+1/2*sin((\x -1) * pi/2 r)});
\addlegendentry{$M_{\theta_0}X_t(\pi/2)$}
\draw[<->](axis cs: 1.5,1)--(axis cs: 1.5,0.7071068)
  node[pos=0.5, right]{\scriptsize $G_{\theta_0}(\pi/2)$};
\draw[<->] (axis cs: 3, -0.70710680-0.05)--(axis cs: 3.5,-0.7071068-0.05) 
  node[pos=0.5, below right]{\scriptsize $\Phi_{\theta_0}(\pi/2)$};
\end{axis}
\end{tikzpicture}
\caption{Smoothing of the time series $X_t=\sin(\omega t)$ by the moving average $M_{\theta_0}X_t=\frac{1}{2}X_{t-1}+\frac{1}{2}X_{t}$ for $\omega=\pi/2$.}\label{fig:exgainPhase}
\end{figure}


## Desirable properties of a moving average

The moving average are often constructed under some specific constraints. 
In the report we will focus on two constraints:

- the preservation of certain kind of trends;

- the variance reduction.

### Trend preservation

Is is often desirable for a moving average to conserve certain kind of trends.
A moving average $M_\theta$ conserve a function of the time $f(t)$ if $\forall t:\:M_\theta f(t)=f(t)$.

We have the following properties for the moving average $M_\theta$:

- To conserve a constant series $X_t=a$ we need
$$
\forall t:M_\theta(X_t)=\sum_{k=-p}^{+f}\theta_kX_{t+k}=\sum_{k=-p}^{+f}\theta_ka=a\sum_{k=-p}^{+f}\theta_k=a
$$
the sum of the coefficients of the moving average $\sum_{k=-p}^{+f}\theta_k$ must then be equal to $1$.

- To conserve a linear trend $X_t=at+b$ we need:
$$
\forall t:\:M_\theta(X_t)=\sum_{k=-p}^{+f}\theta_kX_{t+k}=\sum_{k=-p}^{+f}\theta_k[a(t+k)+b]=at\sum_{k=-p}^{+f}k\theta_k+b\sum_{k=-p}^{+f}\theta_k=at+b
$$
which is equivalent to:
$$
\sum_{k=-p}^{+f}\theta_k=1
\quad\text{and}\quad
\sum_{k=-p}^{+f}k\theta_k=0
$$
- In general, it can be shown that $M_\theta$ conserve a polynomial of degree $d$ if and only if:
$$
\sum_{k=-p}^{+f}\theta_k=1 
 \text{ and } 
\forall j \in \left\llbracket 1,d\right\rrbracket:\:
\sum_{k=-p}^{+f}k^j\theta_k=0
$$
- If $M_\theta$ is symmetric ($p=f$ and $\theta_{-k} = \theta_k$) and conserve polynomial of degree $2d$ then it also conserve polynomial of degree $2d+1$.

### Variance reduction

All time series are affected by noise that can blur the signal extraction.
Hence, we seek to reduce the variance of the noise.
The sum of the sum of the squares of the coefficients $\sum_{k=-p}^{+f}\theta_k^2$ is the *variance reduction* ratio.

Indeed, let $\{\varepsilon_t\}$ a sequence of independent random variables with $\E{\varepsilon_t}=0$, $\V{\varepsilon_t}=\sigma^2$.
$$
\V{M_\theta\varepsilon_t}=\V{\sum_{k=-p}^{+f} \theta_k \varepsilon_{t+k}}
= \sum_{k=-p}^{+f} \theta_k^2 \V{\varepsilon_{t+k}}=
\sigma^2\sum_{k=-p}^{+f} \theta_k^2
$$

## Real-time estimation and asymmetric moving average {#defAsymProb}

For symmetric filters, the phase shift function is equal to zero.
Therefore, there is no delay in any frequency: that's why they are prefered to the asymmetric ones. 
However, they cannot be used in the beginning and in the end of the time series because no past/future value can be used. 
Thus, for real-time estimation, it is needed to build asymmetric moving average that approximate the symmetric moving average.

The approximation is summarised by quality indicators. 
In this paper we focus on the ones defined by @ch15HBSA and @trilemmaWMR2019 to build the asymmetric filters.

@ch15HBSA propose a general approach to derive linear filters, based on an optimization problem of three criteria: *Fidelity* ($F_g$, noise reduction), *Smoothness* ($S_g$) and *Timeliness* ($T_g$, phase shift between input and ouput signals). 
See section \@ref(sec:GuggemosEtAl) for more details.

@trilemmaWMR2019 propose an approach based on the decomposition of the mean squared error between the symmetric and the asymmetric filter in four quantities: *Accuracy* ($A_w$), *Timeliness* ($T_w$), *Smoothness* ($S_w$) and *Residual* ($R_w$).
See section \@ref(sec:WildiMcLeroy) for more details.

All the indicators are summarized in table \@ref(tab:QC).

\begin{table}[!ht]
$$\begin{array}{ccc}
\hline \text{Sigle} & \text{Description} & \text{Formula}\\
\hline b_{c} & \text{constant bias} & \sum_{k=-p}^{+f}\theta_{k}-1\\
\hline b_{l} & \text{linear bias} & \sum_{k=-p}^{+f}k\theta_{k}\\
\hline b_{q} & \text{quadratic bias} & \sum_{k=-p}^{+f}k^{2}\theta_{k}\\
\hline F_{g} & \text{variance reduction / fidelity} & \sum_{k=-p}^{+f}\theta_{k}^{2}\\
\hline S_{g} & \text{Smoothness (Guggemos)} & \sum_{j}(\nabla^{3}\theta_{j})^{2}\\
\hline T_{g} & \text{Timeliness (Guggemos)} & \int_{0}^{2\pi/12}\rho_{\theta}(\omega)\sin(\varphi_{\theta}(\omega))^{2}\ud\omega\\
\hline A_{w} & \text{Accuracy (Wildi)} & 2\int_0^{2\pi/12}\left(\rho_{s}(\omega)-\rho_{\theta}(\omega)\right)^{2}h_{RW}(\omega)\ud\omega\\
\hline T_{w} & \text{Timeliness (Wildi)} & 8\int_0^{2\pi/12} \rho_{s}(\omega)\rho_{\theta}(\omega)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)h_{RW}(\omega)\ud\omega\\
\hline S_{w} & \text{Smoothness (Wildi)} & 2\int_{2\pi/12}^{\pi}\left(\rho_{s}(\omega)-\rho_{\theta}(\omega)\right)^{2}h_{RW}(\omega)\ud\omega\\
\hline R_{w} & \text{Residual (Wildi)} & 8\int_{2\pi/12}^{\pi} \rho_{s}(\omega)\rho_{\theta}(\omega)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)h_{RW}(\omega)\ud\omega\\
\hline \\
\end{array} $$
\caption{Criteria used to check the quality of a linear filter defined by its coefficients $\theta=(\theta_k)_{-p\leq k\leq f}$ and its gain and phase shift function, $\rho_{\theta}$ and $\varphi_\theta$.} 
\label{tab:QC}
\footnotesize
\emph{Note: $X_g$ criteria are derived from \textcite{ch15HBSA} and $X_g$ criteria from \textcite{trilemmaWMR2019}.}

\emph{$\rho_s$ represent the gain function of an Henderson symmetric filter to compare with the asymmetric ones.}

\emph{$h_{RW}$ is the spectral density of a random walk: $h_{RW}(\omega)=\frac{1}{2(1-\cos(\omega))}$.}
\end{table}

# Local polynomial filters {#sec:lppfilters}

In this section we detail the filters that arised from fitting a local polynomial to our time series, as described by @proietti2008.

We assume that our time series $y_t$ can be decomposed as 
$$
y_t=\mu_t+\varepsilon_t
$$
where $\mu_t$ is the signal (trend) and $\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})$ is the noise. 
We assume that $\mu_t$ can be locally approximated by a polynomial of degree $d$ of the time $t$ between $y_t$ and the neighboring observations $\left(y_{t+j}\right)_{j\in\left\llbracket -h,h\right\rrbracket}$. Then $\mu_t\simeq m_{t}$ with:
$$
\forall j\in\left\llbracket -h,h\right\rrbracket :\:
y_{t+j}=m_{t+j}+\varepsilon_{t+j},\quad m_{t+j}=\sum_{i=0}^{d}\beta_{i}j^{i}
$$
This signal extraction problem is then equivalent to the estimation of $m_t=\beta_0$. In matrix notation we can write:
$$
\underbrace{\begin{pmatrix}y_{t-h}\\
y_{t-(h-1)}\\
\vdots\\
y_{t}\\
\vdots\\
y_{t+(h-1)}\\
y_{t+h}
\end{pmatrix}}_{y}=\underbrace{\begin{pmatrix}1 & -h & h^{2} & \cdots & (-h)^{d}\\
1 & -(h-1) & (h-1)^{2} & \cdots & (-(h-1))^{d}\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & h-1 & (h-1)^{2} & \cdots & (h-1)^{d}\\
1 & h & h^{2} & \cdots & h^{d}
\end{pmatrix}}_{X}\underbrace{\begin{pmatrix}\beta_{0}\\
\beta_{1}\\
\vdots\\
\vdots\\
\vdots\\
\vdots\\
\beta_{d}
\end{pmatrix}}_{\beta}+\underbrace{\begin{pmatrix}\varepsilon_{t-h}\\
\varepsilon_{t-(h-1)}\\
\vdots\\
\varepsilon_{t}\\
\vdots\\
\varepsilon_{t+(h-1)}\\
\varepsilon_{t+h}
\end{pmatrix}}_{\varepsilon}
$$
Two parameters are crucial in determining the accuracy of the approximation:

- the degree $d$ of the polynomial;

- the number of neighbored $H=2h+1$ (or the *bandwidth* $h$).

In order to estimate $\beta$ we need $H\geq d+1$ and the estimation is done by the weighted least squares (WLS), which consists of minimizing the following objective function:
$$
S(\hat{\beta}_{0},\dots,\hat{\beta}_{d})=\sum_{j=-h}^{h}\kappa_{j}(y_{t+j}-\hat{\beta}_{0}-\hat{\beta}_{1}j-\dots-\hat{\beta}_{d}j^{d})^{2}
$$
where $\kappa_j$ is a set of weights called *kernel*. We have $\kappa_j\geq 0$, $\kappa_{-j}=\kappa_j$ and with $K=diag(\kappa_{-h},\dots,\kappa_{h})$, the estimate of $\beta$ can be written as $\hat{\beta}=(X'KX)^{1}X'Ky$. 
With $e_{1}=\begin{pmatrix}1&0&\cdots&0\end{pmatrix}'$, the estimate of the trend is:
$$
\hat{m}_{t}=e_{1}\hat{\beta}=w'y=\sum_{j=-h}^{h}w_{j}y_{t-j}\text{ with }w=KX(X'KX)^{-1}e_{1}
$$
To conclude, the estimate of the trend $\hat{m}_{t}$ can be obtained applying the symmetric filter $w$ to $y_t$^[
$w$ is symmetric due to the symmetry of the kernel weights $\kappa_j$.
].
Moreover, $X'w=e_{1}$ so:
$$
\sum_{j=-h}^{h}w_{j}=1,\quad\forall r\in\left\llbracket 1,d\right\rrbracket :\sum_{j=-h}^{h}j^{r}w_{j}=0
$$
Hence, the filter $w$ preserve deterministic polynomial of order $d$.

## Different kernels {#sec:kernels}

In signal extraction, we generally look for weighting observations according to their distance from time $t$: this is the role of the kernel function.
For that, we introduce a kernel function $\kappa_j$, $j=0,\pm1,\dots,\pm h$ with $\kappa_j \geq0$ and $\kappa_j=\kappa_{-j}$.
An important class of kernels is the Beta kernels. In the discrete, up to a proportional factor (so that $\sum_{j=-h}^h\kappa_j=1$):
$$
\kappa_j = \left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^r
\right)^s
$$
with $r>0$, $s\geq 0$. 
It encompass all kernels used in this report, except Henderson, trapezoidal and gaussian kernel.The following kernels are considered in this report:


\newpage 

\begin{multicols}{2}
\begin{itemize}
\item $r=1,s=0$ uniform kernel: 
$$\kappa_j^U=1$$
\item $r=s=1$ triangle kernel:
$$\kappa_j^T=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert
\right)$$

\item $r=2,s=1$  Epanechnikov (or Parabolic) kernel:
$$\kappa_j^E=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
\right)$$

\item $r=s=2$ biweight kernel:
$$\kappa_j^{BW}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
\right)^2$$

\item $r = 2, s = 3$ triweight kernel:
$$\kappa_j^{TW}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
\right)^3$$

\item $r = s = 3$ tricube kernel:
$$\kappa_j^{TC}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^3
\right)^3$$

\item Henderson kernel (see section \ref{sec:sympolyfilter} for more details):
$$
\kappa_{j}=\left[1-\frac{j^2}{(h+1)^2}\right]
\left[1-\frac{j^2}{(h+2)^2}\right]
\left[1-\frac{j^2}{(h+3)^2}\right]
$$
\item Trapezoidal kernel:
$$
\kappa_j^{TP}=
\begin{cases}
  \frac{1}{3(2h-1)} & \text{ if }j=\pm h 
  \\
  \frac{2}{3(2h-1)} & \text{ if }j=\pm (h-1)\\
  \frac{1}{2h-1}& \text{ otherwise}
\end{cases}
$$
\item Gaussian kernel\footnote{
In this report we arbirarily take $\sigma^2=0.25$.
}
$$
\kappa_j^G=\exp\left(
-\frac{
  j^2
}{
  2\sigma^2h^2
}\right)
$$
\end{itemize}
\end{multicols}

<!-- Let $x\in ]0,1[$ and $f_x(a,b)=\left(1-x^{a}\right)^{b}$. We have: -->
<!-- \begin{align*} -->
<!-- \frac{\partial}{\partial a}f(a,b) &=-a\ln (x)x^a(1-x^{a})^{b}>0 \\ -->
<!-- \frac{\partial}{\partial b}f(a,b)&=\ln(1-x^{a})(1-x^{a})^{b} <0 -->
<!-- \end{align*} -->
<!-- So: -->

Henderson, trapezoidal and gaussian kernel are very specific:

- The kernel Henderson and trapezoidal function change with the bandwidth (the other kernel only depend on the ratio $j/h+1$).

- Other definitions of the trapezoidal and gaussian kernel can be used. 
The trapezoidal kernel is here considered because it corresponds to the filter used to extract the seasonal component in the X-12-ARIMA algorithm. 
Therefore it is never used to extract trend-cycle component.

The figure \@ref(fig:kernels) summarises the coefficients of the different kernels.
Analysing the coefficients we can already anticipate some properties of the associated filters:

- The triweight kernel has the narrowest distribution.
The narrowest a distribution is, the smallest the weights of furthest neighbors are: the associated filter should have a high weight in the current observation ($t$).

- For $h$ high the Henderson kernel is equivalent to the triweight kernel (since $h+1\sim h+2 \sim h+3$, $\kappa_j^H\sim\kappa_j^{TW}$), the associated filter should also be equivalent.
However, for $h$ small ($h\leq10$) the Henderson kernel is closer to the biweight kernel than to the triweight kernel.

\begin{figure}[!ht]
\animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/kernels/}{2}{30} 
\caption{Coefficients of the different kernels for $h$ from 2 to 30.}\label{fig:kernels}\footnotesize
\emph{Note: to see the animation the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader. 
Otherwise you will only be able to see the results for $h=2$.}
\end{figure}


### Specific symmetric filters {#sec:sympolyfilter}

When $p=0$ (local adjustment by a constant) we obtain the **Nadaraya-Watson**'s estimator.

With the uniform kernel we obtain the **Macaulay filter**. When $p=0,1$, this is the arithmetic moving average: $w_j=w=\frac{1}{2h+1}$.

The **Epanechnikov** kernel is often recommended as the optimal kernel that minimize the mean square error of the estimation by local polynomial.

**Loess** is a locally weighted polynomial regression that use tricube kernel.

The **Henderson filter** is a specific case of a local cubif fit ($p=3$), widely used for trend estimation (for example it's the filter used in the seasonal adjustment software X-12-ARIMA). For a fixed bandwidth, Henderson found the kernel that gave the smoothest estimates of the trend. 
He showed that the three following problems were equivalent:

1. minimize the variance of third difference of the series by the application of the moving average;  
2. minimize the sum of squares of third difference of the coefficients of the filter, it's the *smothness criterion*: $S=\sum_j(\nabla^{3}\theta_{j})^{2}$;  
3. fit a local cubic polynomial by weighted least squares, where the weights are chose to minimize the sum of squares of the resulting filter.

Resolving the last problem leads to the kernel presented in section \@ref(sec:kernels).


### Analysis of symmetric filters

In this section, all the filters are computed by local polynomial of degree $d=3$. 
The figure \@ref(fig:filterscoefs) plots the coefficients of the filters for the differents kernels presented in different kernels presented in section \@ref(sec:kernels) and for different bandwidth $h$. 
The table \@ref(tab:varianceReductionSymmetricFilters) shows the variance reduction of the different filters.
We find the similar results than in section \@ref(fig:kernels):

- The triweight kernel gives the filter with the narrowest distribution.
The narrowest a distribution is, the higher the variance reduction should be.
Indeed, the distribution of the coefficients of the filter can be interpreted as the output signal of an additive outlier.
As a result, with a wide distribution, an additive outlier will be more persistent than with a narrow distribution.
Therefore, it's the triweight that has the higher variance reduction for all $h\leq30$.

- For $h$ small, the trapezoidal filter seems to produce similar results than the Epanechnikov one.

- For $h$ small the Henderson filter is closed to the biweight kernel, for $h$ high it is equivalent to the triweight kernel.

\begin{figure}[!ht]
\animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/symmetricFilters/}{2}{30}
\caption{Coefficients of symmetric filters computed by local polynomial of degree $3$, according to the differents kernels and for $h$ from 2 to 30.}\label{fig:filterscoefs}\footnotesize
\emph{Note: to see the animation the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader.
Otherwise you will only be able to see the results for $h=2$.}
\end{figure}

Moreover, we find that for all the filters, the coefficients decrease, when the distance to the central observation increases, until a negative value and then increase towards 0 (except for the uniform kernel).
Negative coefficients might be disturbing but they arise from the cubic polynomial constraints.
Indeed to preserve polynomial of degree 2 (and so 3) we need $\sum_{j=-h}^hj^2\theta_i=0$, which constraint some coefficients to be negative.
However, those negative coefficients are negligible compare to the central coefficients (they are more 80% smaller than the central coefficient for all kernels, except for uniform and trapezoidal with high bandwidth).


```{r varianceReductionSymmetricFilters, echo = FALSE}
dataVRSF <- readRDS("data/var_red_sym_filters.RDS")
colnames(dataVRSF)[1] <- "$h$"
title <- "Variance reduction ratio ($\\sum\\theta_i^2$) of symmetric filters computed by local polynomial of degree $3$."
round(dataVRSF[dataVRSF[,1]%in%c(2:10,20,30),],2) %>% 
  kable(format.args = list(digits = 2), align = "c", booktabs = T, row.names = FALSE,
        escape = FALSE,caption = title) %>% 
  kable_styling(latex_options=c("striped", "scale_down", "hold_position")) %>%
  add_header_above(c(" " = 1, "Kernel" = ncol(dataVRSF)-1))
```

### Gain functions 

Figure \@ref(fig:filtersSymgains) plots the gain functions of the different filters. 
Gain functions are usually plotted between 0 and $\pi$. 
However, locally weighted polynomial regression are low-pass filters: they leave almost unchanged low frequency components (such as the trend) and attenuate high frequency fluctuations (noise). 
For a monthly data, a cycle of 3 years correspond to the frequency $2\pi/36$ and a cycle of 7 years to the frequency  $2\pi/84$.
Therefore, an ideal pass-band filter will have a gain function equal to 1 for low frequency ($\leq 2\pi/36$) and equal to 0 for other frequencies.

When the bandwidth $h$ increases, the gain function decreases for low frequencies: short business cycles will then be attenuated.
For a fixed value of $h$, gaussian, Henderson and triweight filters will preserve more short business cycles than the others filters (especially uniform, trapezoidal and Epanechnikov). 
Moreover, the gain function of those filters decreases faster to zero with less fluctuations: it enhances the higher variance reduction ratio shown in table \@ref(tab:varianceReductionSymmetricFilters).

 
\begin{figure}[!ht]
\animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/symmetricFilters/gain}{2}{30}
\caption{Gain functions from 0 to $2\pi/12$ of symmetric filters computed by local polynomial of degree $3$, according to the differents kernels and for $h$ from 2 to 30.}\label{fig:filtersSymgains}\footnotesize
\emph{Note: the two horizontal lines corresponds to the frequencies $2\pi/84$ (cycle of 7 years) and $2\pi/36$ (cycle of 3 years).}

\emph{To see the animation the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader.
Otherwise you will only be able to see the results for $h=2$.}
\end{figure}

Just analysing the symmetric filters properties, there is no doubt that Henderson, triweight and biweight filters have similar properties and will perform better than the other kernel for trend-cycle extraction. 
The same results are found with asymmetric filters. 
Thus, in order to simplify the presentation analysis, in the next sections we will only show the results with the Henderson filter.


## Asymmetric filters

### Direct asymmetric filters (DAF)

As mentionned in section \@ref(defAsymProb), symmetric filters cannot be used in boundary points. For real-time estimation, three different approaches can be used:

1. Build a asymmetric filter fitting local polynomial to the available observations $y_{t}$ for $t\in\left\llbracket n-h,n\right\rrbracket$.

2. Apply the symmetric filter to the series extended by forecast (or backcast) $\hat{y}_{n+l\mid n},l\in\left\llbracket 1,h\right\rrbracket$.

3. Build a asymmetric filter which minimize the mean square revision error subject to polynomial reproducing constraints.

@proietti2008 show that the first two approaches are equivalent when the forecast is done by a polynomial extrapolation of order $d$ (forecasts generated with the same polynomial model than the symmetric filter). This is called the *direct asymmetric filter* (DAF).
Let $q$ be the number of available observations in the future: $q$ varies from 0 (real time filter) to $h$ (symmetric filter).

Rewritting the matrix $X$, $K$ $y$ in the following way:
$$
X=\begin{pmatrix}X_{p}\\
X_{f}
\end{pmatrix},\quad y=\begin{pmatrix}y_{p}\\
y_{f}
\end{pmatrix},\quad K=\begin{pmatrix}K_{p} & 0\\
0 & K_{f}
\end{pmatrix}
$$
where $y_{p}$ correspond to the available data and $y_{f}$ the missing data. The DAF $w_a$ and the forecast $\hat{y}_{f}$ can be written as:
$$
w_{a}=K_{p}X_{p}(X'_{p}K_{p}X_{p})^{-1}e_{1},
\quad
\hat{y}_{f}=X_{f}(X'_{p}K_{p}X_{p})^{-1}X_{p}'K_{p}y_{p}
$$
Moreover, we have the following results with the DAF $w_a$:

- it satisfy the same polynomial reproduction constraints as the symmetric filter (conserve polynomial of degree $d$). 
Thus, the bias in estimating an unknown function of time has the same order of magnitude as in the interior of time support.

- $w_a$ minimize the weighted distance (by the kernel function) between the asymmetric filter coefficients and the symmetric ones. 
Therefore, for the DAF it is equivalent to fit a local polynomial and to minimize the revisions

However, the weights $w_{a,0}$ of the DAF are highly concentrated in the current observation $t$ with an important change between $q=0$ (real-time filter) and $q=h$ (see figure \@ref(fig:filtersdafcoefs)). 
Moreover the real-time filter doesn't have a satisfying gain functions: it is closer to one for all the frequencies (it thus have a low noise reduction power).
Therefore, even if the real-time filter is unbiased (if the series is generated by a polynomial of degree $d$) it is at the expenses of a high variance. 

\begin{figure}[!ht]
\includegraphics[width=\textwidth]{img/daf/coef_gain_1}
\caption{Coefficients and gain function of direct asymmetric filters (DAF) computed by local polynomial of degree $3$ with the Henderson kernel for $h=6$.}\label{fig:filtersdafcoefs}\footnotesize
\end{figure}

<!-- \begin{figure}[!ht] -->
<!-- \animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/daf/coef_gain_}{1}{9} -->
<!-- \caption{Coefficients and gain function of direct asymmetric filters (DAF) computed by local polynomial of degree $3$ with the Henderson kerne for $h=6$.}\label{fig:filtersdafcoefs}\footnotesize -->
<!-- \emph{Note: to see the animation the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader. -->
<!-- Otherwise you will only be able to see the results for the Henderson kernel.} -->
<!-- \end{figure} -->

For all the kernels, we find the same results as in @proietti2008:

- For a fixed value of $d$, the more the data is available ($q$ increases), the more the weight associated to the current observation $w_{a,0}$ decreases.

- For a fixed value of $h$ and $q$, $w_{a,0}$ increases exponentially with the polynomial degree $d$ (in particular, for $d=h$, $w_{a,0}=1$).


### General class of asymmetric filters

To deal with the problem of the variance of the estimates of the real-time filters, @proietti2008 suggest a general of asymmetric filters to make a tradeoff between bias and variance.

Here we consider that the data is generated by the model:
$$
y=U\gamma+Z\delta+\varepsilon,\quad
\varepsilon\sim\mathcal{N}(0,D)
$$
The goal is to find a filter $v$ which minimize the mean square revision error (with the symmetric filter $w$) subject to some constraints. 
The constraints are summarized by the matrix $U=\begin{pmatrix}U_{p}'&U_{f}'\end{pmatrix}'$ (with $U_p$ the available observations of the matrix $U$ for the asymmetric filter): $U_p'v=U'w$. 
The problem is equivalent to find $v$ that minimize:
$$
\varphi(v)=
\underbrace{
  \underbrace{(v-w_{p})'D_{p}(v-w_{p})+
  w_{f}'D_{f}w_{f}}_\text{revision error variance}+
  \underbrace{[\delta'(Z_{p}'v-Z'w)]^{2}}_{biais^2}
}_\text{Mean square revision error}+
\underbrace{2l'(U_{p}'v-U'w)}_{\text{constraints}}
$$
with $l$ a vector of Lagrange multipliers.

When $U=X$ this is equivalent to the constraint to preserve polynomial of degree $d$: we find the direct asymmetric filters $w_a$ with $D=K^{-1}$.

When $U=\begin{pmatrix}1&\cdots&1\end{pmatrix}'$, $Z=\begin{pmatrix}-h&\cdots&+h\end{pmatrix}'$, $\delta=\delta_1$, $D=\sigma^2I$ and when the symmetric filter is the Henderson filter we obtain the Musgrave asymmetric filters (see \colorbox{BurntOrange}{ajouter biblio}). 
With the filter we assume that the data is generated by a linear process and that the asymmetric filters preserve constant signals ($\sum v_i=\sum w_i=1$). 
The asymmetric filters depends on the ratio $\delta_1/\sigma$, which is related to the "I-C" ratio $R=\frac{\bar{I}}{\bar{C}}=\frac{\sum\lvert I_t-I_{t-1}\rvert}{\sum\lvert C_t-C_{t-1}\rvert}$ ($\delta_1/\sigma=2/(R\sqrt{\pi})$), the ratio between the expected absolute difference of the irregular and of the trend-cycle.
In the seasonal adjustment method, the I-C ratio^[
To compute the I-C ratio, a first decomposition of the seasonally adjusted series is computed using a 13-term Henderson moving average.
] is used to determine the bandwidth to used for the Henderson filter. For monthly data:

- if $R<1$ a 9-term Henderson is used ($h=4$);

- if $1\leq R\leq3.5$ a 13-term Henderson is used ($h=6$);

- if $3.5< R$ a 23-term Henderson is used ($h=12$).

In this report, for simplicity we only consider 13-term symmetric filters: the ratio  $\delta^2/\sigma^2$ is fixed to $3.5$.

When $U$ corresponds to the first $d^*+1$ first the columns of $X$, $d^*<d$, the constraint is that the asymmetric filter should reproduce polynomial of degree $d^*$, the potential bias depends on the value of $\delta$.
This will reduce the variance at the expense of a bias: it is the idea followed by @proietti2008 to propose three class of asymmetric filters:

1. *Linear-Constant* (LC): $y_t$ linear ($d=1$) and $v$ preserve constant signals ($d^*=0$). We obtain Musgrave filters when the Henderson kernel is used.

2. *Quadratic-Linear* (QL): $y_t$ quadratic ($d=2$), $v$ depend on the ratio $\delta_2^2/\sigma^2$.  

3. *Cubic-Quadratic* (CQ): $y_t$ cubic ($d=3$), $v$ depend on the ratio $\delta_2^2/\sigma^2$.


<!-- ```{r criteriaLp, echo = FALSE} -->
<!-- lp_diagnostics <- readRDS("data/lp_diagnostics.RDS") -->
<!-- title <- "Quality criteria of real-time filters ($q=0$) computed by local polynomial." -->
<!-- groupement <- table(factor(lp_diagnostics[,1],levels = unique(lp_diagnostics[,1]), ordered = TRUE)) -->
<!-- lp_diagnostics[,-1] %>%  -->
<!--   kable(format.args = list(digits = 3), align = "c", booktabs = T, row.names = FALSE, -->
<!--         escape = FALSE,caption = title) %>%  -->
<!--   kable_styling(latex_options=c(#"striped",  -->
<!--                                 "scale_down", "hold_position")) %>% -->
<!--   add_header_above(c(" " = 1, "Kernel" = ncol(lp_diagnostics)-2)) %>% -->
<!--   pack_rows(index = groupement) -->
<!-- ``` -->

The table \@ref(tab:criteriaLp) show the quality criteria of the different methods with the Henderson kernel and $h=6$. 
For real-time filters ($q=0$), the more complex the filter is (in terms of polynomial preservation), the less the timeliness is and the more the fidelity/smoothness is:the reduction of the time-delay is at the expense of an increase variance. 
This change when $q$ increases: for $q=2$ the QL filter has a greater timeliness that the LC filter. 
This unexpected result underlines the fact that in the approach of @proietti2008, the timeliness is never set as a goal to minimize.

Regarding the mean square revision error ($A_w+S_w+T_w+R_w$), LC and QL filters always gives better results than CQ and DAF filters. Similar results are founded applying the different filters to the Industrial production indices of the European Union between 2003 and 2019. 

This suggest to focus on LC and QL filters, i.e.: to build asymmetric linear filters that preserve polynomial trends of degree less than one.

```{r criteriaLp, echo = FALSE}
lp_diagnostics <- readRDS("data/lp_diagnostics_henderson.RDS")
title <- "Quality criteria of asymmetric filters ($q=0,1,2$) computed by local polynomial with Henderson kernel for $h=6$ and $R=3.5$."
groupement <- table(lp_diagnostics[,1])
lp_diagnostics[,-1] %>% 
  kable(format.args = list(digits = 3), align = "c", booktabs = T, row.names = FALSE,
        escape = FALSE,caption = title) %>% 
  kable_styling(latex_options=c(#"striped", 
                                "scale_down", "hold_position")) %>%
  pack_rows(index = groupement, escape = FALSE)
```


The results for the different kernels can also be visualised in an online application available at https://aqlt.shinyapps.io/FiltersProperties/.

:::: {.summary data-latex="{local polynomial filters}"}
**Advantages**:

- Simple models with an easy interpretation.

- The asymmetric linear filter is independent of the data and of the date of estimation.


**Drawbacks**:

- Timeliness is not controlled.
::::



# General optimisation problem: @ch15HBSA {#sec:GuggemosEtAl}

@ch15HBSA defined a general approach to derive linear filters, based on an optimization problem of three criteria:

- *Fidelity*, $F_g$: it's the variance reduction ratio. It is called "Fidelity" become we want the output signal to be as close as possible to the input signal where the noise component is removed
$$
F_g(\theta) = \sum_{k=-p}^{+f}\theta_{k}^{2}
$$

- *Smoothness*, $S_g$: it's the Henderson smoothness criterion (sum of the squared of the third difference of the coefficients of the filter). 
It measures the flexibility of the coefficient curve of a filter and the smoothness of the trend.
$$
S_g(\theta) = \sum_{j}(\nabla^{3}\theta_{j})^{2}
$$

- *Timeliness*, $T_g$: it mesures the phase-shift between input and output signal for specific frequencies. 
When a linear filter is applied, the level input signal is also altered by the gain function. 
Therefore, it is natural to consider that the higher the gain is, the higher the phase shift impact is. 
That's why the timeliness criterion depends on the gain and phase shift functions ($\rho_\theta$ and $\varphi_{\theta}$), the link between both functions being made by a penalty function $f$.
$$
T_g(\theta)=\int_{\omega_{1}}^{\omega_{2}}f(\rho_{\theta}(\omega),\varphi_{\theta}(\omega))\ud\omega
$$
In this article we use $\omega_1=0$ and $\omega_2=2\pi/12$ (for monthly data): we focus on the phase shift impact on cycles of more than one year.
For the penalty function, we take $f\colon(\rho,\varphi)\mapsto\rho^2\sin(\varphi)^2$. 
Indeed, for this function, the timeliness criterion is analytically solvable ($T_g=\theta'T\theta$ with $T$ a square matrix of order $p+f+1$), which is better in a computational point of view.

The asymmetric filters are computed minimizing a weighted sum of the past three criteria, subject to some constraints. Those constraints are usually polynomial preservation.

$$
\begin{cases}
\underset{\theta}{\min} & J(\theta)=
\alpha F_g(\theta)+\beta S_g(\theta)+\gamma T_g(\theta)\\
s.t. & C\theta=a
\end{cases}
$$
The Henderson symmetric filters can for example be computed with 
$$C=\begin{pmatrix}
1 & \cdots&1\\
-h & \cdots&h \\
(-h)^2 & \cdots&h^2
\end{pmatrix},\quad
a=\begin{pmatrix}
1 \\0\\0
\end{pmatrix},\quad
\alpha=\gamma=0,\quad
\beta=1$$

\colorbox{BurntOrange}{Parler de la convexité}

:::: {.summary data-latex="{FST filters}"}
**Advantages**:

- The approach can be customized adding new criteria.

- The asymmetric linear filter is independent of the symmetric filter, the data and the date of estimation.


**Drawbacks**:

- The different criteria are not normalized: the associated weights cannot be compared.
::::

# Data-dependent filter: @trilemmaWMR2019 {#sec:WildiMcLeroy}

In @trilemmaWMR2019, the authors proposed a data-dependent approach to derive linear filters. They decompose the mean square revision error in a trilemma between three quantities : *accuracy*, *timeliness* and *smoothness*.

Let:

- $\left\{ x_{t}\right\}$ be our input time series;

- $\left\{y_{t}\right\}$ the target signal, i.e. the result of a symmetric filter, and $\Gamma_s$, $\rho_s$ and $\varphi_s$ the associated frequency response, gain and phase shift functions.

- $\left\{\hat y_{t}\right\}$ an estimation of $\left\{y_{t}\right\}$, i.e. the result of an asymmetric filter (when not all observations are available), and $\Gamma_\theta$, $\rho_\theta$ and $\varphi_\theta$ the associated frequency response, gain and phase shift functions.

If we assume that $\left\{ x_{t}\right\}$ is weakly stationary with a continuous spectral density $h$, the mean square revision error, $\E{(y_{t}-\hat{y}_{t})^{2}}$, can be written as:
\begin{equation}
\E{(y_{t}-\hat{y}_{t})^{2}}=\frac{1}{2\pi}\int_{-\pi}^{\pi}\left|\Gamma_s(\omega)-{\Gamma_\theta}(\omega)\right|^{2}h(\omega)\ud\omega=\frac{1}{2\pi}\times2\times\int_{0}^{\pi}\left|\Gamma_s(\omega)-{\Gamma_\theta}(\omega)\right|^{2}h(\omega)\ud\omega
(\#eq:msedef)
\end{equation}

This equality can also be generalized to non-stationary integrated process (for example imposing cointegration between both signals and using pseudo-spectral density, see @optimrtfWMR2013).

\begin{align}
\left|\Gamma_s(\omega)-\Gamma_\theta(\omega)\right|^{2} & =\rho_s(\omega)^{2}+\rho_\theta(\omega)^{2}+2\rho_s(\lambda)\rho_\theta(\lambda)\left(1-\cos(\varphi_s(\omega)-\varphi_\theta(\omega)\right) \nonumber\\
 & =\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2}+4\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_s(\omega)-\varphi_\theta(\omega)}{2}\right)
 (\#eq:msedecomp)
\end{align}

The interval $[0,\pi]$ is then splitted in two: the pass-band $[0,\omega_1]$ (the frequency interval that contains the target signal) and the stop-band  $[\omega_1,\pi]$.

The mean squared error defined in equation \@ref(eq:msedef) can then be decomposed additively into four quantities:
\begin{align*}
Accuracy =A_w&= 2\int_0^{\omega_1}\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2}h(\omega)\ud\omega\\
Timeliness =T_w&= 8\int_0^{\omega_1}\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)h(\omega)\ud\omega\\
Smoothness =S_w&= 2\int_{\omega_1}^\pi\left(\rho_s(\omega)^{2}-\rho_\theta(\omega)\right)^{2}h(\omega)\ud\omega\\
Residual =R_w&= 8\int_{\omega_1}^\pi\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)h(\omega)\ud\omega\\
\end{align*}

One of the drawbacks of this method is that there is no guarantee that there is a unique solution. 


:::: {.remark}
The formulas of the four criteria slightly differ from the ones defined in @trilemmaWMR2019 to have coherent definitions between all sections:

- in this paper the interval integrations is $[0,\pi]$ rather than $[-\pi;\pi]$ (the integral are then only multiplied by 2 because all the functions are even);

- the pass-pand interval is defined as the frequency interval that contains the target signals whereas in @trilemmaWMR2019 it depends on the gain function of the symmetric filer (pass-band$=\{\omega |\rho_s(\omega)\geq 0.5\}$).
:::: 


In general, the residual $R_w$ is small since $\rho_s(\omega)\rho_\theta(\omega)$ is close to 0 in the stop-band. 
Moreover, user priorities are rarely concerned about the time-shift properties of components in the stop-band. 
That's why, to derive linear filters the residual is not taken into account and that the authors suggest to compute then minimizing a weighted sum of the first three indicators:
$$
\mathcal{M}(\vartheta_{1},\vartheta_{2})=\vartheta_{1}T_w(\theta)+\vartheta_{2}S_w(\theta)+(1-\vartheta_{1}-\vartheta_{2})A_w(\theta)
$$
One of the drawbacks of this method is that there is no guarantee that there is a unique solution. 


In this paper we focus in non-parametric approaches to derive linear filters. 
That's why this approach is not considered.
However, the decomposition of the mean squared error gave useful indicators to compare linear filters because, in contrary the one presented in section \@ref(sec:GuggemosEtAl), their values can be easily interpreted and compared to each other.

To have criteria that don't depend on the data, we take for the symmetric filter the Henderson filter and we fix the spectral density to the one of a random walk:
$$
h_{RW}(x)=\frac{1}{2(1-\cos(x))}
$$
The formula presented in table \@ref(tab:QC) are then found using the fact that for a symmetric filter the phase shit is equal to zero ($\varphi_s(\omega)=0$).

:::: {.summary data-latex="{Data-dependent filters}"}
**Advantages**:

- The values of the different criteria can be compared: the weight can be easily interpreted.

**Drawbacks**:

- Data-dependent filter: it depends on the symmetric filter, the data and the date of estimation.

- Some optimisation problems might occur (several minimum, etc.).

::::

# Asymmetric filters and Reproducing Kernel Hilbert Space {#sec:Dagum}

@dagumbianconcini2008 developed a general approach to derive linear filters based on Reproducing Kernel Hilbert Space (RKHS) methodology.

A RKHS is a Hilbert space characterized by a kernel that reproduces, via an inner product defined by a density function $f_0(t)$, every function of the space. 
Therefore, a kernel estimator $K_p$ of order $p$ (i.e. : that reproduce without distortion a polynomial trend of degree $p-1$) can be decomposed into the product of a reproducing kernel $R_{p-1}$, belonging to the space of polynomials of degree $p-1$, and a probability density function $f_0$.

For any sequence $\left(P_{i}\right)_{0\leq i\leq p-1}$ of orthonormal polynomials in $\mathbb{L}^{2}(f_{0})$^[
$\mathbb{L}^{2}(f_{0})$ is the Hilbert space defined by the inner product:
$$
\left\langle U(t),V(t)\right\rangle =\E{U(t)V(t)}=\int_{\R}U(t)V(t)f_{0}(t)\ud t
$$
], the kernel estimator $K_p$ is defined by:
$$
K_{p}(t)=\sum_{i=0}^{p-1}P_{i}(t)P_{i}(0)f_{0}(t)
$$
The weight of a symmetric filter are then derived with:
$$
\forall j\in\left\llbracket -h,h\right\rrbracket\::\: w_{j}=\frac{K_p(j/b)}{\sum_{i=-h}^{^h}K_p(i/b)}
$$
where $b$ is a time-invariant global bandwidth parameter.

The density $f_0$ corresponds to the continuous versions of the kernel defined in \@ref(sec:kernels). 
For example, the biweight function is $f_{0B}(t)=(15/16)(1-t^2)^2,t\in [-1,1]$.
The local polynomial filter obtained with the biweight kernel is then obtained using the bandwidth $b=m+1$.

The goal of @dagumbianconcini2008 is to derive asymmetric filters from the Henderson symmetric filter, therefore the ideal kernel function would be the Henderson one. 
However, as shown in section \@ref(sec:kernels), the Henderson density is a function of the bandwidth and needs to be calculated any time $m$ changes (as its corresponding orthonormal polynomial). That's why the authors use the biweight kernel to approximate the Henderson kernel (for $h\geq 24$ they suggest to consider the triweight kernel).

The asymmetric weighted are obtained adapting the kernels to the length of the asymmetric filters:
$$
\forall j\in\left\llbracket -h,p\right\rrbracket\::\: w_{a,j}=\frac{K_p(j/b)}{\sum_{i=-h}^{^p}K_p(i/b)}
$$
With $b=h+1$, @proietti2008 show that we obtain the direct asymmetric filters (DAF).

In @dagumbianconcini2015, the authors suggest to perform an optimal bandwidth selection (parameter $b$), decomposing the mean squared revision error as in equation \@ref(eq:msedecomp) but with a uniform spectarl density ($h(\omega)=1$).
The bandwidth can then be chosen to minimise the mean squared revision error, the phase shift, etc. 
The following bandwidth selection are studied:
\begin{align*}
b_{q,G}&=\underset{b_q\in]h;2 h+1]}{\min}
\sqrt{2\int_{0}^{\pi}
\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2}\ud \omega
}\\
b_{q,\gamma}&=\underset{b_q\in]h;2 h+1]}{\min}
\sqrt{2\int_{0}^{\pi}
\lvert \Gamma_s(\omega)-\Gamma_\theta(\omega)\rvert^2\ud \omega
} \\
b_{q,\varphi}&=\underset{b_q\in]h;2 h+1]}{\min}
8\int_{0}^{2\pi/12}
\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)\ud \omega
\end{align*}

One of the drawbacks of the optimal bandwidth selection is that there is no guarantee that there is a unique solution. 


:::: {.remark}
The formulas of $b_{q,G}$, $b_{q,\gamma}$ and $b_{q,\varphi}$ slightly differ from the ones defined in @dagumbianconcini2015 to have coherent definitions between all sections:

- $b_{q,\varphi}$ is defined as 
$$
b_{q,\varphi}=\underset{b_q\in]h;2 h+1]}{\min}
\sqrt{2\int_{\Omega_S}
\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)\ud \omega}
$$

With $\Omega_S=[0,2\pi/36]$ the frequency domain associated to cycles of 16 months or longer.

- in @dagumbianconcini2015 a different formula is used for the frequency response function ($\Gamma_\theta(\omega)=\sum_{k=-p}^{+f} \theta_k e^{2\pi i \omega k}$), it only changes the interval integration.
::::


The table \@ref(tab:criteriarkhs)

```{r criteriarkhs, echo = FALSE}
rkhs_diagnostics <- readRDS("data/rkhs_diagnostics.RDS")
title <- "Quality criteria of asymmetric filters ($q=0,1,2$) computed by the RKHS methodology $h=6$."
groupement <- table(rkhs_diagnostics[,1])
rkhs_diagnostics[,-1] %>% 
  kable(format.args = list(digits = 3), align = "c", booktabs = T, row.names = FALSE,
        escape = FALSE,caption = title) %>% 
  kable_styling(latex_options=c(#"striped", 
                                "scale_down", "hold_position")) %>%
  pack_rows(index = groupement, escape = FALSE)
```





:::: {.summary data-latex="{RKHS filters}"}
**Advantages**:

- Filters that apply to irregular frequency series (for example with a lot of missing values) can easily be computed.

**Drawbacks**:

- The linear filters don't preserve polynomial trends of degree 1 or more.

- Some optimisation problems might occur (several minimum, etc.).

::::

# Comparison of the different filters

Mettre les graphiques 3D,

https://aqlt.shinyapps.io/FSTfilters/

# Results

\@ref(fig:filtersasymErr):

- almost same symmetric trend for all the filters: small differences during "turning points". 
Differences less than one point for between all the kernels, uniform and trapezoidal more "atypical", Differences less than 0,5 point for the other kernels.
Différences p/r Henderson toujours dans le même sens sauf pour triweight (?) : sous Henderson pendant les "cycles bas", au-dessus pendant "cycles hauts"

- revisions error are higher with uniform and trapezoidal kernels, lower with triweight, regardless of the asymmetric filter.

- in mean, the variance revision error is always lower with the LC filter and higher with the DAF. However, during turning-points (for example around the 2008 crisis), the LC filter have a higher bias than the other filters. For the French IPI, the CQ method seems to give the better results around the 2008 crisis (analyzing mean squared error revision between 2007 and 2011). However, reproducing the same procedure to the IPI of other countries of the European Union (and the United-Kingdom), we find that for most of the series (23 of 28) its the QL filter that gives the better results during the 2008 crisis.

\begin{figure}[!ht]
\animategraphics[autoplay,loop,width=\textwidth,controls]{2}{img/daf/comp_assym_}{1}{9}
\caption{Revision error of the different real-time ($q=0$) asymmetric filters for $h=6$, for the IPI-FR.}\label{fig:filtersasymErr}\footnotesize
\emph{Note: to see the animation the PDF must be open with Acrobat Reader, KDE Okular, PDF-XChange or Foxit Reader.
Otherwise you will only be able to see the results for the Henderson kernel.}
\end{figure}

\newpage

# References



