---
title: "Asymmetric Linear Filters for Seasonal Adjustment, Applications to the Covid-19 Impact in Monthly Time Series"
author:
  - name: "Alain Quartier-la-Tente"
    authsuperscript: 1*
  - name: "Dominique Ladiray"
    authsuperscript: 2
affiliation:
  - affsuperscript: 1
    dptuniv: "LEMNA"
  - affsuperscript: 2
    dptuniv: "Independant"
corrauthor:
    email: alain.quartier@yahoo.fr
abstract: >
    The COVID-19 crisis highlights that business cycle analysis, and in particular the early detection of turning points, is a major topic in the analysis of economic outlook. Moving averages, or linear filters, are ubiquitous in business cycle extraction and seasonal adjustment methods. For example, the X-12ARIMA seasonal adjustment method uses Henderson moving averages and composite moving averages to estimate the main components of a time series, while TRAMO-SEATS uses Wiener-Kolmogorov filters. Symmetric filters are applied to the center of the series, but when it comes to estimate the most recent points, all of these methods must rely on asymmetric filters which introduce phase-shifts and delays in the detection of turning points. Construction of good asymmetric filters, in terms of fidelity, revisions, smoothness and timeliness, is still an open topic. This presentation describes and compares the recent approaches on trend-cycle extraction and asymmetric filters, and applies them to the COVID-19 economic crisis. All the methods are implemented in the \faRProject{} package `rjdfilters` (https://github.com/palatej/rjdfilters) and the results can be easily reproduced.
keywords: [Seasonal adjustment, asymmetric filters, turning points, Covid-19.]
journalinfo: "JSM2021 Draft manuscript"
date: "`r format(Sys.time(), '%d %B %Y')`"
lang: en-US
corrauthorlabel: Corresponding author
bibliography: biblio.bib
biblio-style: chicago
always_allow_html: yes
output:
  bookdown::pdf_book:
    template: latex/template.tex
    citation_package: natbib
    latex_engine: xelatex
    keep_tex: yes
  rmdformats::downcute:
    use_bookdown: yes
    lightbox: yes
  bookdown::gitbook:
    config:
      download: "pdf"
      sharing:
        github: yes
  bookdown::html_document2:
    toc: yes
    toc_float: yes
  bookdown::word_document2: default
---

```{r DoNotModify, include=FALSE}
### Utilities. Do not modify.
# Installation of packages if necessary
InstallPackages <- function(Packages) {
  InstallPackage <- function(Package) {
    if (!Package %in% installed.packages()[, 1]) {
      install.packages(Package, repos="https://cran.rstudio.com/")
    }
  }
  invisible(sapply(Packages, InstallPackage))
}

# Basic packages
InstallPackages(c("bookdown", "formatR", "kableExtra", "ragg"))

# kableExtra must be loaded 
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  # Word output (https://stackoverflow.com/questions/35144130/in-knitr-how-can-i-test-for-if-the-output-will-be-pdf-or-word)
  # Do not use autoformat (https://github.com/haozhu233/kableExtra/issues/308)
  options(kableExtra.auto_format = FALSE)
}
options(kableExtra.latex.load_packages = TRUE)
library("kableExtra")

# Chunk font size hook: allows size='small' or any valid Latex font size in chunk options
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r Options, include=FALSE}
### Customized options for this document
# Add necessary packages here
Packages <- c( "tidyverse")
# Install them
InstallPackages(Packages)

# knitr options
knitr::opts_chunk$set(
  cache=FALSE, # Cache chunk results
  echo = FALSE, # Show/Hide R chunks
  warning=FALSE, # Show/Hide warnings
  # Figure alignment and size
  fig.align='center', out.width='80%',
  # Graphic devices (ragg_png is better than standard png)
  dev = c("ragg_png", "pdf"),
  # Code chunk format
  tidy=TRUE, tidy.opts=list(blank=FALSE, width.cutoff=50),
  size="scriptsize", knitr.graphics.auto_pdf = TRUE
  )
options(width=50)
load(file = "tables.RData")
```


# Introduction


Moving averages, or linear filters, are ubiquitous in business cycle extraction and seasonal adjustment methods. 
For example, the X-12ARIMA seasonal adjustment method uses Henderson moving averages and composite moving averages to estimate the main components of a time series, while TRAMO-SEATS uses Wiener-Kolmogorov filters. 
Symmetric filters are applied to the center of the series, but when it comes to estimate the most recent points (i.e.: for real time estimates), all of these methods must rely on asymmetric filters. 
For example, even if X-12ARIMA or TRAMO-SEATS apply symmetrical averages to the forecasts obtained from an ARIMA model of the series, in reality it consists in applying asymmetric filters at the end of the series, because the predicted values are linear combinations of past values.

If the classic asymmetric moving averages have good properties regarding the future revisions induced by the process^[See for example @pierce1980SA.], they create phase shifts that impact the real-time estimation of turning points, introducing time delay in the detection.

This presentation is part of an ongoing research on the description and comparison of recent approaches around trend-cycle extraction and asymmetric filters: local polynomial filters of @proietti2008, filters based on Reproducing Kernel Hilbert Space detailed by @dagumbianconcini2008 and methods based on an optimization of filters properties of @ch15HBSA or @trilemmaWMR2019

# From seasonal adjustment to trend-cycle estimation {#sec:SAtoTCE}

To perform seasonal adjustment, most of the algorithm decompose the data in several unknown components: trend-cycle, seasonal and irregular component.
In X-12ARIMA and TRAMO-SEATS (the most popular seasonal adjustment methods), prior to the decomposition, the initial series is pre-adjustment from deterministic effects (outliers, calendar effects).
Thus, the estimation of trend-cycle component is technically linked to the seasonal component.

Moreover, since the trend-cycle extraction methods are applied to seasonally adjusted data, both problems cannot be separated. In this paper, we study  trend-cycle extraction methods that can be implemented in the seasonal adjustment method X-12ARIMA.
This is why almost all the methods described are implemented in the core libraries of JDemetra+ ^[See https://github.com/jdemetra/jdemetra-core.], the seasonal adjustment software recommended by Eurostat. 
An \faIcon{r-project} interface is implemented in the package `rjdfilters`^[Available at https://github.com/palatej/rjdfilters.].

In the literature, different approaches were considered for trend-cycle extraction^[See for example @alexandrov2012TEreview for a review of some modern approaches to trend extraction.]. 
Among the most popular, we can cite the Model-Based Approach and the non-parametric extraction methods: 

- The Model-Based Approach assumes the specification of a stochastic time series model for the trend (ARIMA model, state space model, etc.) and the estimates is obtained by minimizing a penalty function, generally the mean squared error.
This is for example the case of the Kalman filter, the Wiener-Kolmogorov filter (used in TRAMO-SEATS) and the Direct Filter Approach of @trilemmaWMR2019 (section \@ref(sec:WildiMcLeroy)).

- The non-parametric extraction methods do not assume that the structure of a model is fixed and can be easily applied to any time series. 
This is for example the case of the Henderson and Musgrave filters (used in X-12ARIMA).
Classical methods can be seen as local polynomial regression, approach generalized by @proietti2008 (section \@ref(sec:lppfilters)).  
Non-parametric estimators can also be reproduced by exploiting Reproducing Kernel Hilbert Space (RKHS) methodology, like it is done by 
@dagumbianconcini2008 (section \@ref(sec:Dagum)). 

@ch15HBSA (section \@ref(sec:GuggemosEtAl)) drawn a general unifying approach that allows a theoretical link between Musgrave non-parametric filter and the Direct Filter Approach.
They also proposed a global procedure to build asymmetric moving averages, that allows to minimize the phase shift effects and which is described in this report.


# Basic properties of moving average

In this section we describe some definitions and properties of moving average and linear filters (see for example @ch12HBSA for more details).

Let $p$ et $f$ two integers, a moving average $M_\theta$ or $M$ is defined by a set of coefficients $\theta=(\theta_{-p},\dots,\theta_{f})'$ such as for all time series $X_t$:
$$
M_\theta(X_t)=\sum_{k=-p}^{+f}\theta_kX_{t+k}
$$

- $p+f+1$ is called the *moving average order*.

- When $p=f$ the moving average is said to be *centered*. 
If we also have $\forall k:\:\theta_{-k} = \theta_k$, the moving average $M_\theta$ is said to be *symmetric*. 
In this case, the quantity $h=p=f$ is called the *bandwidth*.

Let $X_t=\e^{-i\omega t}$. The result of the moving average $M_\theta$ in $X_t$ is:
$$
Y_t = M_{\theta}X_t = \sum_{k=-p}^{+f} \theta_k \e^{-i \omega (t+k)}
= \left(\sum_{k=-p}^{+f} \theta_k \e^{-i \omega k}\right)\cdot X_t.
$$
The function $\Gamma_\theta(\omega)=\sum_{k=-p}^{+f} \theta_k e^{-i \omega k}$ is called the *transfer function* or *frequency response function*.
It can be rewritten as:
$$
\Gamma_\theta(\omega) = G_\theta(\omega)\e^{-i\Phi_\theta(\omega)}
$$
where $G_\theta(\omega)=\lvert\Gamma_\theta(\omega)\rvert$ is the *gain* or *amplitude* function and $\Phi_\theta(\omega)$ is the *phase shift* or *time shift* function. 
For all symmetric moving average we have $\Phi_\theta(\omega)\equiv 0 \pmod{\pi}$.

To sum up, applying a moving average to a harmonic times series affects it in in two different ways:

- by multiplying it by an amplitude coefficient $G_{\theta}\left(\omega\right)$;

- by "shifting" it in time by $\Phi_\theta(\omega)/\omega$, which directly affects the detection of turning points.

# Description of the different approaches

## Local polynomial filters {#sec:lppfilters}

In this subsection we detail the filters that arise from fitting a local polynomial to our time series, as described by @proietti2008. 
Local polynomial filters encompass classical filters like Henderson and Musgrave filters.

### Symmetric filter

We assume that our time series $y_t$ can be decomposed as 
$$
y_t=\mu_t+\varepsilon_t
$$
where $\mu_t$ is the signal (trend) and $\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})$ is the noise^[The time series is therefore seasonally adjusted.]. 
We assume that $\mu_t$ can be locally approximated by a polynomial of degree $d$ of the time $t$ between $y_t$ and the neighboring observations $\left(y_{t+j}\right)_{j\in\left\llbracket -h,h\right\rrbracket}$. 
Then $\mu_t\simeq m_{t}$ with:
$$
\forall j\in\left\llbracket -h,h\right\rrbracket :\:
y_{t+j}=m_{t+j}+\varepsilon_{t+j},\quad m_{t+j}=\sum_{i=0}^{d}\beta_{i}j^{i}
$$
This signal extraction problem is then equivalent to the estimation of $m_t=\beta_0$. In matrix notation we can write:

$$
y=X\beta+\varepsilon,\quad \text{with: }
\begin{cases}
y&=\begin{pmatrix}y_{t-h} & \cdots &y_{t+h}\end{pmatrix}'\\
\beta &=\begin{pmatrix}\beta_1 & \cdots &\beta_d \end{pmatrix}'\\
\varepsilon&=\begin{pmatrix}\varepsilon_{t-h} & \cdots &\varepsilon_{t+h}\end{pmatrix}'\\
\end{cases}
$$
and:
$$
X=\begin{pmatrix}1 & -h & \cdots & (-h)^{d}\\
1 & -(h-1) & \cdots & (-(h-1))^{d}\\
\vdots & \vdots & \cdots & \vdots\\
1 & 0 &  \cdots & 0\\
\vdots  & \vdots & \cdots & \vdots\\
1 & h-1  & \cdots & (h-1)^{d}\\
1 & h  & \cdots & h^{d}
\end{pmatrix}.
$$

The estimation of $\beta$ is done by the weighted least squares (WLS), which consists of minimizing the following objective function:
$$
S(\hat{\beta}_{0},\dots,\hat{\beta}_{d})=\sum_{j=-h}^{h}\kappa_{j}(y_{t+j}-\hat{\beta}_{0}-\hat{\beta}_{1}j-\dots-\hat{\beta}_{d}j^{d})^{2}
$$
where $\kappa_j$ is a set of weights called *kernel*. We have $\kappa_j\geq 0:\kappa_{-j}=\kappa_j$, and with $K=diag(\kappa_{-h},\dots,\kappa_{h})$, the estimate of $\beta$ can be written as $\hat{\beta}=(X'KX)^{1}X'Ky$. 
With $e_{1}=\begin{pmatrix}1&0&\cdots&0\end{pmatrix}'$, the estimate of the trend is:
$$
\hat{m}_{t}=e_{1}\hat{\beta}=w'y=\sum_{j=-h}^{h}w_{j}y_{t-j}\text{ with }w=KX(X'KX)^{-1}e_{1}
$$
The goal of the kernel function is to weight the observations according to their distance from time $t$. There exist many different kernels and its choice can lead to some well-known estimators: Nadaraya-Watson estimator ($d=0$), Macaulay filter, Loess, etc. Here, we will only focus on the kernel that produces the Henderson filter, widely used for trend-cycle estimation (for example it’s the filter used in the seasonal adjustment software X-12ARIMA):
$$\kappa_{j}=\left[1-\frac{j^2}{(h+1)^2}\right]
\left[1-\frac{j^2}{(h+2)^2}\right]
\left[1-\frac{j^2}{(h+3)^2}\right]$$

To conclude, the estimate of the trend $\hat{m}_{t}$ can be obtained applying the symmetric filter $w$ to $y_t$^[
$w$ is symmetric due to the symmetry of the kernel weights $\kappa_j$.
].
Moreover, $X'w=e_{1}$ so:
$$
\sum_{j=-h}^{h}w_{j}=1,\quad\forall r\in\left\llbracket 1,d\right\rrbracket :\sum_{j=-h}^{h}j^{r}w_{j}=0
$$
Hence, the filter $w$ preserve deterministic polynomial of order $d$.


### Asymmetric filter

To deal with the problem of the variance of the estimates of the real-time filters, @proietti2008 suggest a general of asymmetric filters to make a tradeoff between bias and variance.
It is a generalization of Musgrave asymmetric filters (used in the seasonal adjustment algorithm X-12ARIMA, see @musgrave1964set). They propose four classes of asymmetric filters, obtained by minimizing mean square revision error (with the symmetric filter) subject to some constraints:

1. *Direct asymmetric filters* (DAF): the asymmetric filter preserves the same polynomial constraints than the symmetric filter. This is equivalent to apply the symmetric methods to the available observations

2. *Linear-Constant* (LC): $y_t=a+\delta t+\varepsilon_t$, $\varepsilon_t\sim\mathcal N(0,\sigma^2)$ and the asymmetric filter only preserves constant trends. We obtain the so-called Musgrave filters when the Henderson kernel is used.

3. *Quadratic-Linear* (QL): $y_t=a+bt+\delta t^2+\varepsilon_t$, $\varepsilon_t\sim\mathcal N(0,\sigma^2)$ and the asymmetric filter only preserves preserves linear trends.

4. *Cubic-Quadratic* (CQ): $y_t=a+bt+ct^2+\delta t^3+\varepsilon_t$, $\varepsilon_t\sim\mathcal N(0,\sigma^2)$ and the asymmetric filter only preserves preserves quadratic trends.

The last three filters depends on the ratio $\delta^2/\sigma^2$ that can be related to the "I-C" ratio $R=\frac{\bar{I}}{\bar{C}}=\frac{\sum\lvert I_t-I_{t-1}\rvert}{\sum\lvert C_t-C_{t-1}\rvert}$ ($\delta_1/\sigma=2/(R\sqrt{\pi})$), the ratio between the expected absolute difference of the irregular and of the trend-cycle.
In the seasonal adjustment method X-12ARIMA, the I-C ratio is used to determine the bandwidth to use for the Henderson filter.

:::: {.summary data-latex="{Local polynomial filters}"}
**Advantages**:

- Simple models with an easy interpretation. 

- The asymmetric linear filter is independent of the date of estimation, but it depends on the data if we calibrate the filter with the "I-C" ratio.

**Drawbacks**:

- Timeliness is not controlled.

**\faRProject{} function**: `rjdfilters::lp_filter()`.
::::

## Reproducing Kernel Hilbert Space {#sec:Dagum}

Classical non-parametric filters (Henderson, LOESS, Hodrick-Prescott) can be characterized using the Reproducing Kernel Hilbert Space (RKHS) methodology, as described by @dagumbianconcini2008.
They use this theory to derive linear filters and associated asymmetric filters.

A RKHS is a Hilbert space characterized by a kernel that reproduces, via an inner product defined by a density function $f_0(t)$, every function of the space. 
Therefore, a kernel estimator $K_p$ of order $p$ (i.e.: that reproduce without distortion a polynomial trend of degree $p-1$) can be decomposed into the product of a reproducing kernel $R_{p-1}$, belonging to the space of polynomials of degree $p-1$, and a probability density function $f_0$.

For any sequence $\left(P_{i}\right)_{0\leq i\leq p-1}$ of orthonormal polynomials in $\mathbb{L}^{2}(f_{0})$^[
$\mathbb{L}^{2}(f_{0})$ is the Hilbert space defined by the inner product:
$$
\left\langle U(t),V(t)\right\rangle =\mathbb E\left[U(t)V(t)\right]=\int_{\mathbb R}U(t)V(t)f_{0}(t)d t
$$
], the kernel estimator $K_p$ is defined by:
$$
K_{p}(t)=\sum_{i=0}^{p-1}P_{i}(t)P_{i}(0)f_{0}(t)
$$
The weights of a symmetric filter are then derived with:
$$
\forall j\in\left\llbracket -h,h\right\rrbracket\::\: w_{j}=\frac{K_p(j/b)}{\sum_{i=-h}^{^h}K_p(i/b)}
$$
where $b$ is a time-invariant global bandwidth parameter.

The goal of @dagumbianconcini2008 is to derive asymmetric filters from the Henderson symmetric filter, therefore the ideal kernel function would be the Henderson one. 
However, the Henderson density is a function of the bandwidth and needs to be calculated any time $m$ changes (as its corresponding orthonormal polynomial). 
That's why the authors use the biweight kernel to approximate the Henderson kernel (for $h\geq 24$ they suggest considering the triweight kernel).

The asymmetric weighted are obtained adapting the kernels to the length of the asymmetric filters:
$$
\forall j\in\left\llbracket -h,q\right\rrbracket\::\: w_{a,j}=\frac{K_p(j/b)}{\sum_{i=-h}^{^q}K_p(i/b)}
$$

In @dagumbianconcini2015, the authors suggest performing an optimal bandwidth selection (parameter $b$). It can for example be chosen minimizing the mean squared revision error ($b_{q,\gamma}$) or the timeliness ($b_{q,\varphi}$):
\begin{align*}
b_{q,\gamma}&=\underset{b_q}{\min}
\sqrt{2\int_{0}^{\pi}
\lvert \Gamma_s(\omega)-\Gamma_\theta(\omega)\rvert^2d \omega
} \\
b_{q,\varphi}&=\underset{b_q}{\min}
\int_{0}^{2\pi/12}
\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)d \omega
\end{align*}
Where $\Gamma_s$ and $\rho_s$ are the transfer function and the gain function of the symmetric filter and $\Gamma_\theta$, $\rho_\theta$ and $\varphi_\theta$ are the transfer function, the gain function and the phase shift function of the asymmetric filter that depend on the bandwidth $b$.

:::: {.summary data-latex="{RKHS filters}"}
**Advantages**:

- The asymmetric linear filter is independent of the data and of the date of estimation.

- Method can be generalized to create filter that could be applied to series with irregular frequency series (for example with a lot of missing values).

**Drawbacks**:

- With some parameters there are several minima that lead to very different results (for example with $h=6$ and $q=5$).

**\faRProject{} function**: `rjdfilters::rkhs_filter()`.
::::


## General optimization problem: FST filters {#sec:GuggemosEtAl}

@ch15HBSA defined a general approach to derive linear filters, based on an optimization problem of three criteria. 
Contrary to the local polynomial approach of @proietti2008, this method enables to control the size of the timeliness in order to minimize the phase-shift.

The following criteria are used:

- *Fidelity*, $F_g$: it's the variance reduction ratio. It is called "Fidelity" because we want the output signal to be as close as possible to the input signal where the noise component is removed
$$
F_g(\theta) = \sum_{k=-p}^{+f}\theta_{k}^{2}
$$
$F_g$ can be rewritten in a positive quadratic form: $F_g(\theta)=\theta'F\theta$ with $F$ the identity matrix of order $p+f+1$.

- *Smoothness*, $S_g$: it's the Henderson smoothness criterion (sum of the squared of the third difference of the coefficients of the filter). 
It measures the flexibility of the coefficient curve of a filter and the smoothness of the trend.
$$
S_g(\theta) = \sum_{j}(\nabla^{3}\theta_{j})^{2}
$$
$S_g$ could also be rewritten in a positive quadratic form: $S_g(\theta)=\theta'S\theta$ with $S$ a symmetric matrix of order $p+f+1$.


- *Timeliness*, $T_g$: it measures the phase shift between input and output signal for specific frequencies. 
When a linear filter is applied, the level input signal is also altered by the gain function. 
Therefore, it is natural to consider that the higher the gain is, the higher the phase shift impact is. 
That's why the timeliness criterion depends on the gain and phase shift functions ($\rho_\theta$ and $\varphi_{\theta}$), the link between both functions being made by a penalty function $f$.
$$
T_g(\theta)=\int_{0}^{2\pi/12}f(\rho_{\theta}(\omega),\varphi_{\theta}(\omega))\ud\omega
$$
For the penalty function, the authors suggest taking $f\colon(\rho,\varphi)\mapsto\rho^2\sin(\varphi)^2$. 
Indeed, for this function, the timeliness criterion is analytically solvable ($T_g=\theta'T\theta$ with $T$ a square symmetric matrix of order $p+f+1$), which is better in a computational point of view.

The asymmetric filters are computed minimizing a weighted sum of the past three criteria, subject to some constraints. Those constraints are usually polynomial preservation.

$$
\begin{cases}
\underset{\theta}{\min} & J(\theta)=
\alpha F_g(\theta)+\beta S_g(\theta)+\gamma T_g(\theta)\\
s.t. & C\theta=a
\end{cases}
$$
The conditions $\alpha,\beta,\gamma\geq 0\text{ and }\alpha\beta\ne 0$ guarantee that $J(\theta)$ is a strictly convex function: therefore the optimization problem has a unique solution.

The Henderson symmetric filters can for example be computed with 
$$C=\begin{pmatrix}
1 & \cdots&1\\
-h & \cdots&h \\
(-h)^2 & \cdots&h^2
\end{pmatrix},\quad
a=\begin{pmatrix}
1 \\0\\0
\end{pmatrix},\quad
\alpha=\gamma=0,\quad
\beta=1$$

:::: {.summary data-latex="{FST filters}"}
**Advantages**:

- The asymmetric linear filter is independent of the symmetric filter, the data and the date of estimation.

- Unique solution to the optimization problem.

- The approach can be customized adding new criteria.

**Drawbacks**:

- The different criteria are not normalized: the associated weights cannot be compared.

**\faRProject{} function**: `rjdfilters::fst_filter()`.
::::

## Data-dependent filter {#sec:WildiMcLeroy}

In @trilemmaWMR2019, the authors proposed a data-dependent approach to derive linear filters. They decompose the mean square revision error in a trilemma between three quantities: *accuracy*, *timeliness* and *smoothness*. 
Contrary to @ch12HBSA, this decomposition implies that the values of the three criteria are comparable.

Let:

- $\left\{ x_{t}\right\}$ be our input time series;

- $\left\{y_{t}\right\}$ the target signal, i.e. the result of a symmetric filter, and $\Gamma_s$, $\rho_s$ and $\varphi_s$ the associated frequency response, gain and phase shift functions.

- $\left\{\hat y_{t}\right\}$ an estimation of $\left\{y_{t}\right\}$, i.e. the result of an asymmetric filter (when not all observations are available), and $\Gamma_\theta$, $\rho_\theta$ and $\varphi_\theta$ the associated frequency response, gain and phase shift functions.

If we assume that $\left\{ x_{t}\right\}$ is weakly stationary with a continuous spectral density $h$, the mean square revision error, $\E{(y_{t}-\hat{y}_{t})^{2}}$, can be written as:
\begin{align}
&\E{(y_{t}-\hat{y}_{t})^{2}}=\frac{1}{2\pi}\int_{-\pi}^{\pi}\left|\Gamma_s(\omega)-{\Gamma_\theta}(\omega)\right|^{2}h(\omega)\ud\omega \nonumber
\\&\quad=\frac{1}{2\pi}\times2\times\int_{0}^{\pi}\left|\Gamma_s(\omega)-{\Gamma_\theta}(\omega)\right|^{2}h(\omega)\ud\omega
(\#eq:msedef)
\end{align}
This equality can also be generalized to non-stationary integrated process (for example imposing cointegration between both signals and using pseudo-spectral density, see @optimrtfWMR2013).

We have:
\begin{align*}
&\left|\Gamma_s(\omega)-\Gamma_\theta(\omega)\right|^{2}=\rho_s(\omega)^{2}+\rho_\theta(\omega)^{2} + \\
&\qquad \phantom{=}2\rho_s(\lambda)\rho_\theta(\lambda)\left(1-\cos(\varphi_s(\omega)-\varphi_\theta(\omega)\right) \\
 &\qquad =\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2}+\\
&\qquad \phantom{=}4\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_s(\omega)-\varphi_\theta(\omega)}{2}\right)
\end{align*}

The interval $[0,\pi]$ is then split in two: the pass-band $[0,\omega_1]$ (the frequency interval that contains the target signal) and the stop-band  $[\omega_1,\pi]$.

The mean squared error defined in equation \@ref(eq:msedef) can then be decomposed additively into four quantities, accuracy, timeliness, smoothness and residual:
\begin{gather*}
A_w= 2\int_0^{\omega_1}\left(\rho_s(\omega)-\rho_\theta(\omega)\right)^{2}h(\omega)\ud\omega\\
T_w= 8\int_0^{\omega_1}\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)h(\omega)\ud\omega\\
S_w= 2\int_{\omega_1}^\pi\left(\rho_s(\omega)^{2}-\rho_\theta(\omega)\right)^{2}h(\omega)\ud\omega\\
R_w= 8\int_{\omega_1}^\pi\rho_s(\lambda)\rho_\theta(\lambda)\sin^{2}\left(\frac{\varphi_\theta(\omega)}{2}\right)h(\omega)\ud\omega\\
\end{gather*}


In general, the residual $R_w$ is small since $\rho_s(\omega)\rho_\theta(\omega)$ is close to 0 in the stop-band. 
Moreover, user priorities are rarely concerned about the time-shift properties of components in the stop-band. 
That's why, to derive linear filters the residual is not taken into account; and that's why the authors suggest minimizing a weighted sum of the first three indicators:
$$
\mathcal{M}(\vartheta_{1},\vartheta_{2})=\vartheta_{1}T_w(\theta)+\vartheta_{2}S_w(\theta)+(1-\vartheta_{1}-\vartheta_{2})A_w(\theta)
$$

:::: {.summary data-latex="{Data-dependent filters}"}
**Advantages**:

- The values of the different criteria can be compared: the weights can be easily interpreted.

**Drawbacks**:

- Data-dependent filter: it depends on the symmetric filter, the data and the date of estimation.

- Some optimization problems might occur (several minimum, etc.).

**\faRProject{} implementation**: package `MDFA`   
(https://github.com/wiaidp/MDFA).
::::

For the moment, this method is not compared to the others: the filter is totally data-dependent, so it can be difficult to implement it in the seasonally adjusted method X-12ARIMA. Moreover, due to the high degree of freedom ("customization triangle" of $\mathcal{M}(\vartheta_{1},\vartheta_{2})$), it would need more studied to decide which parameter to choose.


# Comparison of the different methods

## Comparison with the FST approach

The FST approach provides a useful tool to validate a method of construction of linear filter. 
Indeed, a linear filter can be considered as suboptimal if we can find a set of weight for the FST approach that gives better results (in terms of fidelity, smoothness and timeliness) with the same (or higher) polynomial constraints.

For $h=6$ (13-term symmetric filter), for the RKHS filters we find that:

- imposing asymmetric filters to preserve constants, the FST approach gives better results for all values of $q$ for the filters computed with $b_{q,G}$ and $b_{q,\gamma}$, and for $q\leq 3$ for the one computed with $b_{q,\varphi}$.

- imposing asymmetric filters to preserve linear trends, the FST approach gives better results for $q\in[2,5]$ for the filters computed with $b_{q,G}$ and $b_{q,\gamma}$, and for $q\in[2,4]$ for the one computed with $b_{q,\varphi}$.

\faArrowCircleRight{} In terms of quality criteria, FST approach seems to perform better than the RKHS approach to build asymmetric filters to apply to monthly regular data (i.e.: data for which we usually use $h=6$).

For the local polynomial filters we find that:

- imposing asymmetric filters to preserve constants, the FST approach gives better results than the LC method for all values of $q$. 

- imposing asymmetric filters to preserve linear trends, the FST approach gives better results for $q\in[2,5]$ for the LC and QL methods. 

\faArrowCircleRight{} In terms of quality criteria, local polynomial seems to perform better for real-time and one-period ahead filters ($q=0,1$).

Those comparisons can be seen in an online application available at https://aqlt.shinyapps.io/FSTfilters/.

## Comparison in term of turning point detection

### Methodology

In this section, we compare the filters in terms of delay to detect turning points.
A turning point (upturn) is defined to occur at time $t$ if $y_{t-k}\geq\cdots\geq y_{t-1}<y_t\leq y_{t+1}\leq\cdots y_{t+m}$. 
A downturn would be equivalently defined as a date $t$ $y_{t-k}\leq\cdots\leq y_{t-1}>y_t\geq y_{t+1}\geq\cdots y_{t+m}$.
Following @Zellner1991, $k=3$ and $m=1$ are chosen given the smoothness of the trend-cycle data. 

We used the calendar adjusted Eurostat database on monthly industrial production indices (`sts_inpr_m`, 2 404 series). The turning points are determined on the trend-cycle component computed with the seasonal adjustment method X-12ARIMA on the overall series. The time delay is defined as the number of months necessary to detect the correct turning point without any further revisions.

For each series and each date, the following methodology is used:

1. The series is seasonally adjusted with X-12ARIMA to extract: the linearized component, the length of the trend and seasonal filters, the decomposition scheme and the I-C ratio

2. The seasonal adjusted is done on the linearized component with the X-11 algorithm, using the same parameters as before but with a different trend-cycle filter and without using the forecasts of the pre-adjustment^[
This can be done with the \faRProject{} functions `rjdfilters::x11_lp()`, `rjdfilters::x11_rkhs()` and `rjdfilters::x11_fst()`.
]. To control the impact of the estimates of the parameters of the Reg-ARIMA model, the linearized series used here is the one computed on the overall series.

3. For each estimate of the trend, we compute the downturns and the upturns.

Due to the high degree of freedom of the FST approach, for the moment we only compared the X-12ARIMA approach to local polynomial and RKHS filters^[
Further research will be done on the FST approach to see which set of weights performs better. One lead could be the study the set of weights that produces linear filters with better quality criteria than RKHS or local polynomial filters.
]. For the RKHS filters, we only retain the ones that minimize the timeliness because in our case we want to minimize the time delay of turning point detection.

### Results

The table \@ref(tab:covid-quantile) compares the different methods for the turning points detection during the Covid-19 crisis (i.e.: during the year 2020). The LC method (i.e.: Musgrave filters) seems to perform, in distribution, better than the other methods (and have similar performance than the X-12ARIMA method) and the less performant method seems to be the RKHS filters obtained by minimizing the timeliness. This could be explained by the issue underlined in section \@ref(sec:Dagum): for the RKHS filters we can have several extrema in the optimization problem that lead to different asymmetric filters. The worst performance of QL and CQ filters compared to LC could be explained by the highest volatility of the estimates due to the strongest constraints on the polynomial trend conservation.

Figures \@ref(fig:C235DEp1) to \@ref(fig:C25SEp2) show two examples of trend-cycle estimates around the turning point of February 2020. For those series, the real time trend-cycle estimates produced by the methods QL, CQ and DAF seem unrealistic (too high rise or fall), and the ones produce by RKHS filters seems too noisy. X-12ARIMA and Musgrave filters (LC) seems to produce very similar results.

The same statistics can be computed in over periods, such as the global financial crisis of 2007–2008 (table \@ref(tab:fc-quantile)). We observe the same hierarchy between the methods: the best results are obtained with the LC filters and X-12ARIMA. What can be surprising is that for almost all the series, the time delay to detect turning points without any further revisions is above 6 months, which is the number of months necessary to use the symmetric filter (the results are only displayed for the series that uses filters of order 13). This will be investigated in further studies, but it could be explained by the impact of outliers, detected during crisis that are corrected in the pre-adjustment and during the decomposition.

# First conclusions

In this ongoing research, we describe and compare some modern approach to build asymmetric filters to minimize revision and time shift in the detection of turning points that could be used in the seasonal adjustment method X-12ARIMA.

From this first results, we can conclude that there is no need to build asymmetric filters which preserve polynomial trends of degree more than one (QL, CQ, DAF filters):  it introduces more variance on the estimates (and thus more revisions), without any gain in term of detection of turning point. This implies that, in further simulations on the FST approach, we can focus on the one that preserves, at best, polynomial linear trends.

This study could be extended in many ways. For example, we could study other modern approaches, such as the Direct Filter Approach, or study the impact of atypical points (and thus new kinds of asymmetric filters based on robust methods). For the JSM presentation, we plan to investigate deeper the results presented in this draft manuscript.


```{r covid-quantile}
caption <- "Deciles of the time delay to detect turning points (downturn and upturn) in 2020 (900 observations)."
footnote <- "Statistics computed on series for which the optimal trend-cycle filter is of length 13."
D2 <- "D2: 20% of the turning points are detected with less than 3 months for LC filter and less than 5 months for RKHS filter." 
kbl(table_covid,
    caption = caption,
    booktabs = TRUE,
    align = c("c",rep("c", ncol(table_covid))))  %>% 
    kable_styling(latex_options = c("striped")) %>% 
    footnote(c(footnote, D2),
             threeparttable = TRUE)
```

```{r fc-quantile}
caption <- "Deciles of the time delay to detect turning points (downturn and upturn) between 2007 and 2009 (2 535 observations)."
footnote <- "Only series for which the optimal trend-cycle filter is of length 13."
D2 <- "D2: 20% of the turning points are detected with less than 15 months for LC filter and X-12ARIMA and less than 16 months for other methods." 
kbl(table_fc,
    caption = caption,
    booktabs = TRUE,
    align = c("c",rep("c", ncol(table_fc))))  %>% 
    kable_styling(latex_options = c("striped", "hold_position")) %>% 
    footnote(c(footnote, D2),
             threeparttable = TRUE)
```



\begin{figure*}\centering
\includegraphics[width=0.95\textwidth]{img/C235DE_x13}
\includegraphics[width=0.95\textwidth]{img/C235DE_rkhs} 
\includegraphics[width=0.95\textwidth]{img/C235DE_lc}
\caption{Estimates of the trend-cycle component for the industrial production index in the manufacture of cement, lime and plaster (C235) in Germany (turning point in February 2020).}
\label{fig:C235DEp1}
\end{figure*}

\begin{figure*}\centering
\includegraphics[width=0.95\textwidth]{img/C235DE_ql} 
\includegraphics[width=0.95\textwidth]{img/C235DE_cq}
\includegraphics[width=0.9\textwidth]{img/C235DE_daf} 
\caption{Estimates of the trend-cycle component for the industrial production index in the manufacture of cement, lime and plaster (C235) in Germany (turning point in February 2020).}
\label{fig:C235DEp2}
\end{figure*}


\begin{figure*}\centering
\includegraphics[width=0.9\textwidth]{img/C25SE_x13}
\includegraphics[width=0.9\textwidth]{img/C25SE_rkhs} 
\includegraphics[width=0.9\textwidth]{img/C25SE_lc}
\caption{Estimates of the trend-cycle component for the industrial production index in the manufacture of fabricated metal products, except machinery and equipment (C25) in Sweden (turning point in February 2020).}
\label{fig:C25SEp1}
\end{figure*}

\begin{figure*}\centering
\includegraphics[width=0.9\textwidth]{img/C25SE_ql} 
\includegraphics[width=0.9\textwidth]{img/C25SE_cq}
\includegraphics[width=0.9\textwidth]{img/C25SE_daf}
\caption{Estimates of the trend-cycle component for the industrial production index in the manufacture of fabricated metal products, except machinery and equipment (C25) in Sweden (turning point in February 2020).}
\label{fig:C25SEp2}
\end{figure*}

`r if (!knitr:::is_latex_output()) '# References {-}'`
